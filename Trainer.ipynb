{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ef3752",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T08:23:39.384197Z",
     "iopub.status.busy": "2025-04-07T08:23:39.383985Z",
     "iopub.status.idle": "2025-04-07T08:23:40.252724Z",
     "shell.execute_reply": "2025-04-07T08:23:40.251872Z"
    },
    "papermill": {
     "duration": 0.874427,
     "end_time": "2025-04-07T08:23:40.254365",
     "exception": false,
     "start_time": "2025-04-07T08:23:39.379938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "\n",
    "class YahtzeeEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Yahtzee environment that follows the OpenAI Gym interface.\n",
    "    Modified to work better with standard RL algorithms by using discrete spaces\n",
    "    and providing a flattened action space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(YahtzeeEnv, self).__init__()\n",
    "        \n",
    "        # Define scoring categories and their maximum scores\n",
    "        self.categories = {\n",
    "            'ones': 5,         # Max score: 5 (1×5)\n",
    "            'twos': 10,        # Max score: 10 (2×5)\n",
    "            'threes': 15,      # Max score: 15 (3×5)\n",
    "            'fours': 20,       # Max score: 20 (4×5)\n",
    "            'fives': 25,       # Max score: 25 (5×5)\n",
    "            'sixes': 30,       # Max score: 30 (6×5)\n",
    "            'three_of_a_kind': 30,\n",
    "            'four_of_a_kind': 30,\n",
    "            'full_house': 25,\n",
    "            'small_straight': 30,\n",
    "            'large_straight': 40,\n",
    "            'yahtzee': 50,\n",
    "            'chance': 30\n",
    "        }\n",
    "        \n",
    "        # Flatten action space into a single discrete space\n",
    "        # Actions 0-12: Choose scoring category\n",
    "        # Actions 13-44: Reroll combinations (2^5 = 32 possible reroll combinations)\n",
    "        self.action_space = spaces.Discrete(45)\n",
    "        \n",
    "        # Define observation space using MultiDiscrete\n",
    "        self.observation_space = spaces.MultiDiscrete([\n",
    "            6, 6, 6, 6, 6,  # dice values (0-5 representing 1-6)\n",
    "            2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # available categories (binary) (6-18) (13 categories)\n",
    "            3  # remaining rolls (0,1,2) (19)\n",
    "        ])\n",
    "        \n",
    "        # Initialize game state\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.dice = self._roll_dice()\n",
    "        self.remaining_rolls = 2\n",
    "        self.available_categories = np.ones(13, dtype=np.int8)\n",
    "        self.scores = np.zeros(13, dtype=np.int32)\n",
    "        self.total_score = 0\n",
    "        self.is_bonus = False\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def _roll_dice(self, reroll_mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"Roll the dice according to reroll mask.\"\"\"\n",
    "        if reroll_mask is None:\n",
    "            return np.random.randint(0, 6, size=5)  # 0-5 representing 1-6\n",
    "        \n",
    "        new_dice = self.dice.copy()\n",
    "        for i, reroll in enumerate(reroll_mask):\n",
    "            if reroll:\n",
    "                new_dice[i] = np.random.randint(0, 6)\n",
    "        return new_dice\n",
    "    \n",
    "    def _decode_action(self, action: int) -> Tuple[int, np.ndarray]:\n",
    "        \"\"\"Convert flat action space to category and reroll mask.\"\"\"\n",
    "        if action < 13:  # Scoring actions\n",
    "            return action, np.zeros(5, dtype=np.int8)\n",
    "        else:  # Reroll actions\n",
    "            reroll_idx = action - 13\n",
    "            return -1, np.array([int(x) for x in format(reroll_idx, '05b')])\n",
    "    \n",
    "    def _check_bonus(self) -> int:\n",
    "        \"\"\"Check if bonus is earned and return bonus score.\"\"\"\n",
    "        upper_section_score = np.sum(self.scores[:6])\n",
    "        if not self.is_bonus and upper_section_score >= 63:\n",
    "            self.is_bonus = True\n",
    "            return 35\n",
    "        return 0\n",
    "    \n",
    "    def _calculate_score(self, category_idx: int, dice: np.ndarray) -> int:\n",
    "        \"\"\"Calculate score for given category and dice combination.\"\"\"\n",
    "        dice = dice + 1  # Convert from 0-5 to 1-6\n",
    "        dice_counts = np.bincount(dice, minlength=7)\n",
    "        category_name = list(self.categories.keys())[category_idx]\n",
    "        \n",
    "        if category_name in ['ones', 'twos', 'threes', 'fours', 'fives', 'sixes']:\n",
    "            number = category_idx + 1\n",
    "            return (number * dice_counts[number]) + self._check_bonus()\n",
    "        \n",
    "        elif category_name == 'three_of_a_kind':\n",
    "            if np.any(dice_counts >= 3):\n",
    "                return np.sum(dice)\n",
    "            return 0\n",
    "        \n",
    "        elif category_name == 'four_of_a_kind':\n",
    "            if np.any(dice_counts >= 4):\n",
    "                return np.sum(dice)\n",
    "            return 0\n",
    "        \n",
    "        elif category_name == 'full_house':\n",
    "            if np.any(dice_counts == 3) and np.any(dice_counts == 2):\n",
    "                return 25\n",
    "            return 0\n",
    "        \n",
    "        elif category_name == 'small_straight':\n",
    "            for straight in [(1,2,3,4), (2,3,4,5), (3,4,5,6)]:\n",
    "                if all(dice_counts[s] >= 1 for s in straight):\n",
    "                    return 30\n",
    "            return 0\n",
    "        \n",
    "        elif category_name == 'large_straight':\n",
    "            if (all(dice_counts[1:7] == 1) or all(dice_counts[2:8] == 1)) or \\\n",
    "               (all(dice_counts[1:6] == 1) or all(dice_counts[2:7] == 1)):\n",
    "                return 40\n",
    "            return 0\n",
    "        \n",
    "        elif category_name == 'yahtzee':\n",
    "            if np.any(dice_counts == 5):\n",
    "                return 50\n",
    "            return 0\n",
    "        \n",
    "        elif category_name == 'chance':\n",
    "            return np.sum(dice)\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"Take a step in the environment using the given action.\"\"\"\n",
    "        category, reroll = self._decode_action(action)\n",
    "        info = {}\n",
    "            \n",
    "        # Handle reroll action\n",
    "        if category == -1:\n",
    "            if self.remaining_rolls > 0:\n",
    "                self.dice = self._roll_dice(reroll)\n",
    "                self.remaining_rolls -= 1\n",
    "                reward = 0  # Neutral reward for rerolling\n",
    "                return self._get_observation(), reward, False, False, info\n",
    "                \n",
    "            else:\n",
    "                reward = -50  # Penalty for invalid reroll\n",
    "                return self._get_observation(), reward, False, False, info\n",
    "        \n",
    "        # Handle scoring action\n",
    "        if not self.available_categories[category]:\n",
    "            return self._get_observation(), -50, False, False, {'error': 'Category already used'}\n",
    "        \n",
    "        # Calculate score and update state\n",
    "        score = self._calculate_score(category, self.dice)\n",
    "        self.scores[category] = score\n",
    "        self.available_categories[category] = 0\n",
    "        self.total_score += score\n",
    "        \n",
    "        # Reset dice and rolls for next turn\n",
    "        self.dice = self._roll_dice()\n",
    "        self.remaining_rolls = 2\n",
    "        \n",
    "        # Check if game is done\n",
    "        done = np.sum(self.available_categories) == 0\n",
    "        \n",
    "        # Calculate reward (use the score as the reward)\n",
    "        reward = score\n",
    "        \n",
    "        return self._get_observation(), reward, done, False, {\n",
    "            'total_score': self.total_score,\n",
    "            'scores': self.scores.copy()\n",
    "        }\n",
    "    \n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"Return current observation of the environment.\"\"\"\n",
    "        return np.concatenate([\n",
    "            self.dice,  # 5 dice values (0-5)\n",
    "            self.available_categories,  # 13 binary values\n",
    "            [self.remaining_rolls]  # 1 value (0-2)\n",
    "        ])\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the current state of the game.\"\"\"\n",
    "        if mode == 'human':\n",
    "            print(\"\\nCurrent Dice:\", self.dice + 1)  # Convert back to 1-6 for display\n",
    "            print(\"Remaining Rolls:\", self.remaining_rolls)\n",
    "            print(\"\\nAvailable Categories:\")\n",
    "            for i, (category, available) in enumerate(zip(self.categories.keys(), self.available_categories)):\n",
    "                if available:\n",
    "                    possible_score = self._calculate_score(i, self.dice)\n",
    "                    print(f\"{category}: {possible_score} points possible\")\n",
    "            print(\"\\nScored Categories:\")\n",
    "            for i, (category, score) in enumerate(zip(self.categories.keys(), self.scores)):\n",
    "                if not self.available_categories[i]:\n",
    "                    print(f\"{category}: {score}\")\n",
    "            print(\"\\nTotal Score:\", self.total_score)\n",
    "            if self.is_bonus:\n",
    "                print(\"Bonus achieved: +35 points\")\n",
    "    \n",
    "    def get_legal_actions(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return an indicator (binary) vector of legal actions.\n",
    "        \n",
    "        - For scoring actions (0-12): legal if that category is still available.\n",
    "        - For reroll actions (13-44): legal if remaining_rolls > 0 and at least one die is rerolled.\n",
    "        \"\"\"\n",
    "        legal = np.zeros(self.action_space.n, dtype=np.int8)\n",
    "        # Scoring actions: allowed only if the category hasn't been used yet.\n",
    "        for i in range(13):\n",
    "            if self.available_categories[i]:\n",
    "                legal[i] = 1\n",
    "        # Reroll actions: allowed only if there are remaining rolls.\n",
    "        if self.remaining_rolls > 0:\n",
    "            for action in range(13, 45):\n",
    "                # _, reroll = self._decode_action(action)\n",
    "                # if np.any(reroll):  # Must reroll at least one die.\n",
    "                legal[action] = 1\n",
    "        return legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0336e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:23:40.261284Z",
     "iopub.status.busy": "2025-04-07T08:23:40.261049Z",
     "iopub.status.idle": "2025-04-07T08:23:40.288199Z",
     "shell.execute_reply": "2025-04-07T08:23:40.287352Z"
    },
    "papermill": {
     "duration": 0.031803,
     "end_time": "2025-04-07T08:23:40.289408",
     "exception": false,
     "start_time": "2025-04-07T08:23:40.257605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class HardCodedStrategy:\n",
    "    def __init__(self):\n",
    "        self.category_scores = {\n",
    "            'ones': {'type': 'upper', 'number': 1},\n",
    "            'twos': {'type': 'upper', 'number': 2},\n",
    "            'threes': {'type': 'upper', 'number': 3},\n",
    "            'fours': {'type': 'upper', 'number': 4},\n",
    "            'fives': {'type': 'upper', 'number': 5},\n",
    "            'sixes': {'type': 'upper', 'number': 6},\n",
    "            'three_of_a_kind': {'type': 'three_kind'},\n",
    "            'four_of_a_kind': {'type': 'four_kind'},\n",
    "            'full_house': {'type': 'full_house'},\n",
    "            'small_straight': {'type': 'small_straight'},\n",
    "            'large_straight': {'type': 'large_straight'},\n",
    "            'yahtzee': {'type': 'yahtzee'},\n",
    "            'chance': {'type': 'chance'},\n",
    "        }\n",
    "    \n",
    "    def calculate_reroll_strategy(self, dice, target_category, rolls_remaining):\n",
    "        \"\"\"\n",
    "        Optimized reroll strategy based on target category.\n",
    "        Expects dice as list of ints in 0-5 (will be converted to 1-6).\n",
    "        Returns a list of 5 binary values (1 = reroll, 0 = keep).\n",
    "        \"\"\"\n",
    "        if rolls_remaining == 0:\n",
    "            return [0, 0, 0, 0, 0]  # No rerolls left\n",
    "        \n",
    "        # Convert dice from 0-5 to 1-6\n",
    "        dice = [d + 1 for d in dice]\n",
    "        dice_counter = Counter(dice)\n",
    "        \n",
    "        category_info = self.category_scores[target_category]\n",
    "        category_type = category_info['type']\n",
    "        \n",
    "        # Default strategy: reroll all dice\n",
    "        reroll = [1, 1, 1, 1, 1]\n",
    "        \n",
    "        # Upper section (ones through sixes)\n",
    "        if category_type == 'upper':\n",
    "            target_value = category_info['number']\n",
    "            \n",
    "            # Keep all dice of target value\n",
    "            for i, value in enumerate(dice):\n",
    "                if value == target_value:\n",
    "                    reroll[i] = 0\n",
    "\n",
    "        # Three of a Kind\n",
    "        elif category_type == 'three_kind':\n",
    "            most_common = dice_counter.most_common(2)\n",
    "            \n",
    "            # Already have three or more of a kind\n",
    "            if most_common and most_common[0][1] >= 3:\n",
    "                value_to_keep = most_common[0][0]\n",
    "                # Keep the three of a kind\n",
    "                for i, value in enumerate(dice):\n",
    "                    if value == value_to_keep:\n",
    "                        reroll[i] = 0\n",
    "                        \n",
    "                # With remaining dice, keep high values if last roll\n",
    "                if rolls_remaining == 1:\n",
    "                    for i, value in enumerate(dice):\n",
    "                        if value != value_to_keep and value >= 5:\n",
    "                            reroll[i] = 0\n",
    "            \n",
    "            # Have a pair\n",
    "            elif most_common and most_common[0][1] == 2:\n",
    "                value_to_keep = most_common[0][0]\n",
    "                \n",
    "                # If multiple pairs, keep the higher pair\n",
    "                if len(most_common) > 1 and most_common[1][1] == 2:\n",
    "                    if most_common[0][0] < most_common[1][0]:\n",
    "                        value_to_keep = most_common[1][0]\n",
    "                \n",
    "                # Keep the pair\n",
    "                for i, value in enumerate(dice):\n",
    "                    if value == value_to_keep:\n",
    "                        reroll[i] = 0\n",
    "                \n",
    "                # If it's the last roll, also keep high values\n",
    "                if rolls_remaining == 1:\n",
    "                    for i, value in enumerate(dice):\n",
    "                        if reroll[i] == 1 and value >= 5:\n",
    "                            reroll[i] = 0\n",
    "            \n",
    "            # No pairs yet, but last roll - keep highest value\n",
    "            elif rolls_remaining == 1:\n",
    "                highest_value = max(dice) if dice else 6\n",
    "                for i, value in enumerate(dice):\n",
    "                    if value == highest_value:\n",
    "                        reroll[i] = 0\n",
    "                        break\n",
    "\n",
    "        # Four of a Kind\n",
    "        elif category_type == 'four_kind':\n",
    "            most_common = dice_counter.most_common(1)\n",
    "            \n",
    "            # Already have four or more of a kind\n",
    "            if most_common and most_common[0][1] >= 4:\n",
    "                value_to_keep = most_common[0][0]\n",
    "                # Keep the four of a kind\n",
    "                kept = 0\n",
    "                for i, value in enumerate(dice):\n",
    "                    if value == value_to_keep and kept < 4:\n",
    "                        reroll[i] = 0\n",
    "                        kept += 1\n",
    "                        \n",
    "                # With remaining dice, keep high values if last roll\n",
    "                if rolls_remaining == 1:\n",
    "                    for i, value in enumerate(dice):\n",
    "                        if reroll[i] == 1 and value >= 5:\n",
    "                            reroll[i] = 0\n",
    "            \n",
    "            # Have three of a kind\n",
    "            elif most_common and most_common[0][1] == 3:\n",
    "                value_to_keep = most_common[0][0]\n",
    "                # Keep the three of a kind\n",
    "                for i, value in enumerate(dice):\n",
    "                    if value == value_to_keep:\n",
    "                        reroll[i] = 0\n",
    "            \n",
    "            # Have a pair and more rolls remaining\n",
    "            elif most_common and most_common[0][1] == 2:\n",
    "                # With multiple rolls, keep highest pair\n",
    "                high_pair = 0\n",
    "                for val, count in dice_counter.items():\n",
    "                    if count == 2 and val > high_pair:\n",
    "                        high_pair = val\n",
    "                \n",
    "                if high_pair > 0:\n",
    "                    for i, value in enumerate(dice):\n",
    "                        if value == high_pair:\n",
    "                            reroll[i] = 0\n",
    "            \n",
    "            # Last roll and no good combos - keep highest value dice\n",
    "            elif rolls_remaining == 1:\n",
    "                sorted_dice = sorted(enumerate(dice), key=lambda x: x[1], reverse=True)\n",
    "                for idx, _ in sorted_dice[:1]:  # Keep the highest die\n",
    "                    reroll[idx] = 0\n",
    "\n",
    "        # Full House\n",
    "        elif category_type == 'full_house':\n",
    "            # Already have a full house\n",
    "            if len(dice_counter) == 2 and 2 in dice_counter.values() and 3 in dice_counter.values():\n",
    "                reroll = [0, 0, 0, 0, 0]  # Keep all\n",
    "            else:\n",
    "                counts = dice_counter.most_common(2)\n",
    "                \n",
    "                # Have three of a kind and a different pair\n",
    "                if len(counts) == 2 and counts[0][1] >= 3 and counts[1][1] >= 2:\n",
    "                    reroll = [0, 0, 0, 0, 0]  # Keep all\n",
    "                \n",
    "                # Have three of a kind - keep it and try for a pair\n",
    "                elif len(counts) >= 1 and counts[0][1] >= 3:\n",
    "                    three_kind_value = counts[0][0]\n",
    "                    \n",
    "                    # Keep the three of a kind\n",
    "                    for i, value in enumerate(dice):\n",
    "                        if value == three_kind_value:\n",
    "                            reroll[i] = 0\n",
    "                    \n",
    "                    # If we also have a single of a different value and last roll, keep it\n",
    "                    if rolls_remaining == 1 and len(counts) > 1:\n",
    "                        other_value = counts[1][0]\n",
    "                        kept = 0\n",
    "                        for i, value in enumerate(dice):\n",
    "                            if value == other_value and reroll[i] == 1 and kept < 2:\n",
    "                                reroll[i] = 0\n",
    "                                kept += 1\n",
    "                \n",
    "                # Have two pairs - keep both pairs\n",
    "                elif len(counts) >= 2 and counts[0][1] == 2 and counts[1][1] == 2:\n",
    "                    for i, value in enumerate(dice):\n",
    "                        if value == counts[0][0] or value == counts[1][0]:\n",
    "                            reroll[i] = 0\n",
    "                \n",
    "                # Have one pair - keep it\n",
    "                elif len(counts) >= 1 and counts[0][1] == 2:\n",
    "                    pair_value = counts[0][0]\n",
    "                    \n",
    "                    # Keep the pair\n",
    "                    for i, value in enumerate(dice):\n",
    "                        if value == pair_value:\n",
    "                            reroll[i] = 0\n",
    "                    \n",
    "                    # If last roll and we have a single of another value, keep it too\n",
    "                    if rolls_remaining == 1 and len(counts) > 1:\n",
    "                        other_values = [val for val, _ in counts[1:]]\n",
    "                        highest_other = max(other_values)\n",
    "                        kept = 0\n",
    "                        for i, value in enumerate(dice):\n",
    "                            if value == highest_other and reroll[i] == 1 and kept < 1:\n",
    "                                reroll[i] = 0\n",
    "                                kept += 1\n",
    "\n",
    "        # Small Straight\n",
    "        elif category_type == 'small_straight':\n",
    "            values_set = set(dice)\n",
    "            \n",
    "            # Check how close we are to each large straight\n",
    "            low_straight = [1, 2, 3, 4]\n",
    "            mid_straight = [2, 3, 4, 5]\n",
    "            high_straight = [3, 4, 5, 6]\n",
    "            \n",
    "            low_matches = []\n",
    "            mid_matches = []\n",
    "            high_matches = []\n",
    "            for value in values_set:\n",
    "                if value in low_straight:\n",
    "                    low_matches.append(value)\n",
    "                if value in high_straight:\n",
    "                    high_matches.append(value)\n",
    "                if value in mid_straight:\n",
    "                    mid_matches.append(value)\n",
    "                    \n",
    "            low_matches = list(set(low_matches))\n",
    "            high_matches = list(set(high_matches))\n",
    "            mid_matches = list(set(mid_matches))\n",
    "            \n",
    "            maxm = max(len(low_matches), len(mid_matches), len(high_matches))\n",
    "            maxm_list = []\n",
    "            if len(low_matches) == maxm:\n",
    "                maxm_list = low_matches\n",
    "            elif len(mid_matches) == maxm:\n",
    "                maxm_list = mid_matches\n",
    "            elif len(high_matches) == maxm: \n",
    "                maxm_list = high_matches\n",
    "                                \n",
    "            for die in dice:\n",
    "                if die in maxm_list:\n",
    "                    reroll[dice.index(die)] = 0\n",
    "                    maxm_list.remove(die)  # Remove to avoid duplicates\n",
    "\n",
    "        # Large Straight\n",
    "        elif category_type == 'large_straight':\n",
    "            values_set = set(dice)\n",
    "            \n",
    "            # Already have a large straight\n",
    "            if (all(v in values_set for v in [1, 2, 3, 4, 5]) or \n",
    "                all(v in values_set for v in [2, 3, 4, 5, 6])):\n",
    "                reroll = [0, 0, 0, 0, 0]  # Keep all\n",
    "            else:\n",
    "                # Check how close we are to each large straight\n",
    "                low_straight = [1, 2, 3, 4, 5]\n",
    "                high_straight = [2, 3, 4, 5, 6]\n",
    "                \n",
    "                low_matches = []\n",
    "                high_matches = []\n",
    "                for value in values_set:\n",
    "                    if value in low_straight:\n",
    "                        low_matches.append(value)\n",
    "                    if value in high_straight:\n",
    "                        high_matches.append(value)\n",
    "                low_matches = list(set(low_matches))\n",
    "                high_matches = list(set(high_matches))\n",
    "                \n",
    "                if len(low_matches) >= len(high_matches):\n",
    "                    for die in dice:\n",
    "                        if die in low_matches:\n",
    "                            reroll[dice.index(die)] = 0\n",
    "                            low_matches.remove(die)  # Remove to avoid duplicates\n",
    "                else:\n",
    "                    for die in dice:\n",
    "                        if die in high_matches:\n",
    "                            reroll[dice.index(die)] = 0\n",
    "                            high_matches.remove(die)  # Remove to avoid duplicates\n",
    "\n",
    "        # Yahtzee\n",
    "        elif category_type == 'yahtzee':\n",
    "            \n",
    "            most_common = dice_counter.most_common(1)\n",
    "            for i, value in enumerate(dice):\n",
    "                if most_common and most_common[0][0] == value:\n",
    "                    reroll[i] = 0\n",
    "        \n",
    "\n",
    "        # Chance - keep high values\n",
    "        elif category_type == 'chance':\n",
    "            # Always keep 6s\n",
    "            for i, value in enumerate(dice):\n",
    "                if value == 6:\n",
    "                    reroll[i] = 0\n",
    "            \n",
    "            # Keep 5s\n",
    "            for i, value in enumerate(dice):\n",
    "                if value == 5 and reroll[i] == 1:\n",
    "                    reroll[i] = 0\n",
    "            \n",
    "            # On last roll, also keep 4s\n",
    "            if rolls_remaining == 1:\n",
    "                for i, value in enumerate(dice):\n",
    "                    if value == 4 and reroll[i] == 1:\n",
    "                        reroll[i] = 0\n",
    "            \n",
    "            # If we're keeping too few dice and it's the last roll, keep 3s too\n",
    "            if rolls_remaining == 1 and sum(1 for r in reroll if r == 0) <= 2:\n",
    "                for i, value in enumerate(dice):\n",
    "                    if value == 3 and reroll[i] == 1:\n",
    "                        reroll[i] = 0\n",
    "                        \n",
    "        return reroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7be84fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:23:40.295962Z",
     "iopub.status.busy": "2025-04-07T08:23:40.295706Z",
     "iopub.status.idle": "2025-04-07T08:23:43.689296Z",
     "shell.execute_reply": "2025-04-07T08:23:43.688599Z"
    },
    "papermill": {
     "duration": 3.398738,
     "end_time": "2025-04-07T08:23:43.690833",
     "exception": false,
     "start_time": "2025-04-07T08:23:40.292095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque, Counter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim, dim)\n",
    "        self.linear2 = nn.Linear(dim, dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        nn.init.kaiming_normal_(self.linear1.weight)\n",
    "        nn.init.kaiming_normal_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.activation(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        out += identity  # Residual connection\n",
    "        return self.activation(out)\n",
    "\n",
    "class TargetIntuitionNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=13):\n",
    "        super(TargetIntuitionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.res_block = ResidualBlock(128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.res_block(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class YahtzeeAgent:\n",
    "    def __init__(self, env, device):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        \n",
    "        # Calculate input dimension from the environment\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        # Enhanced features as in the original implementation (obs + 22 derived features)\n",
    "        self.enhanced_dim = obs_dim + 22\n",
    "        self.reroll_input_size = 5 * 6 + 13 + 1  # dice one-hot (30) + category one-hot (13) + rolls_remaining (1)\n",
    "        \n",
    "        # Target intuition network (replaces high_level_net)\n",
    "        # This outputs the expected value of targeting each category\n",
    "        self.target_intuition_net = TargetIntuitionNet(self.enhanced_dim, 13).to(self.device)\n",
    "        # self.reroll_net = RerollNet(self.reroll_input_size).to(self.device)\n",
    "\n",
    "        self.reroller = HardCodedStrategy()\n",
    "\n",
    "        # reroll_path = r\"/kaggle/input/reroll_pretained/pytorch/default/1/reroll_net_pretrained.pth\"\n",
    "        # self.reroll_net.load_state_dict(torch.load(reroll_path, map_location=self.device))\n",
    "        # self.reroll_net.to(self.device)\n",
    "        \n",
    "        # Optimizer for the target intuition network\n",
    "        self.optimizer = optim.Adam(self.target_intuition_net.parameters(), lr=0.00005)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 1.0  # Full credit for future rewards (no discount)\n",
    "        self.epsilon = 0.5  # Higher starting epsilon for more exploration\n",
    "        self.epsilon_min = 1.2\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.batch_size = 256  # Larger batch size for more stable learning\n",
    "        self.buffer = deque(maxlen=200000)  # Larger buffer for more diverse experiences\n",
    "        \n",
    "        # Category names for reference\n",
    "        self.categories = [\n",
    "            'ones', 'twos', 'threes', 'fours', 'fives', 'sixes',\n",
    "            'three_of_a_kind', 'four_of_a_kind', 'full_house',\n",
    "            'small_straight', 'large_straight', 'yahtzee', 'chance'\n",
    "        ]\n",
    "        \n",
    "        # Category score descriptions for the reroll strategy\n",
    "        self.category_scores = {\n",
    "            0: {'type': 'upper', 'number': 1},  # Ones\n",
    "            1: {'type': 'upper', 'number': 2},  # Twos\n",
    "            2: {'type': 'upper', 'number': 3},  # Threes\n",
    "            3: {'type': 'upper', 'number': 4},  # Fours\n",
    "            4: {'type': 'upper', 'number': 5},  # Fives\n",
    "            5: {'type': 'upper', 'number': 6},  # Sixes\n",
    "            6: {'type': 'three_kind', 'score': 'sum'},  # Three of a Kind\n",
    "            7: {'type': 'four_kind', 'score': 'sum'},   # Four of a Kind\n",
    "            8: {'type': 'full_house', 'score': 25},     # Full House\n",
    "            9: {'type': 'small_straight', 'score': 30},  # Small Straight\n",
    "            10: {'type': 'large_straight', 'score': 40}, # Large Straight\n",
    "            11: {'type': 'yahtzee', 'score': 50},        # Yahtzee\n",
    "            12: {'type': 'chance', 'score': 'sum'}       # Chance\n",
    "        }\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.stats = {\n",
    "            'upper_bonus_achieved': 0,\n",
    "            'total_games': 0,\n",
    "            'category_usage': {cat: 0 for cat in self.categories},\n",
    "            'scores': [],\n",
    "            'target_switches': 0,  # Track how often the target category changes\n",
    "            'final_category_matches_target': 0  # Track if final category matches initial target\n",
    "        }\n",
    "        \n",
    "        # Timing information\n",
    "        self.times = {\n",
    "            'select_action': [],\n",
    "            'optimize_model': []\n",
    "        }\n",
    "\n",
    "    def enhance_observation(self, observation):\n",
    "        \"\"\"Add derived features to the observation to help the agent learn better\"\"\"\n",
    "        dice = observation[:5] + 1  # Convert 0-5 to 1-6\n",
    "        categories_available = observation[5:18]\n",
    "        rolls_remaining = observation[18]\n",
    "        \n",
    "        # Dice value counts\n",
    "        dice_counts = np.zeros(6)\n",
    "        for i in range(5):\n",
    "            if 1 <= dice[i] <= 6:\n",
    "                dice_counts[int(dice[i])-1] += 1\n",
    "        \n",
    "        # Key statistics about dice\n",
    "        has_three_kind = int(any(count >= 3 for count in dice_counts))\n",
    "        has_four_kind = int(any(count >= 4 for count in dice_counts))\n",
    "        has_yahtzee = int(any(count == 5 for count in dice_counts))\n",
    "        has_pair = int(any(count >= 2 for count in dice_counts))\n",
    "        \n",
    "        # Upper section scoring potential\n",
    "        upper_potentials = np.zeros(6)\n",
    "        for i in range(6):\n",
    "            if categories_available[i] == 1:  # Category is available\n",
    "                upper_potentials[i] = (i+1) * dice_counts[i]\n",
    "        \n",
    "        # Upper section bonus tracking (need 63+ for bonus)\n",
    "        upper_filled = sum(1 for i in range(6) if categories_available[i] == 0)\n",
    "        remaining_turns = sum(categories_available)\n",
    "        \n",
    "        # Calculate straight potential\n",
    "        unique_values = sum(1 for count in dice_counts if count > 0)\n",
    "        small_straight_potential = 1.0 if unique_values >= 4 else (unique_values / 4.0)\n",
    "        large_straight_potential = 1.0 if unique_values >= 5 else (unique_values / 5.0)\n",
    "        \n",
    "        # Full house potential\n",
    "        has_three = any(count == 3 for count in dice_counts)\n",
    "        has_two = any(count == 2 for count in dice_counts)\n",
    "        full_house_potential = 1.0 if (has_three and has_two) else 0.5 if has_three or has_two else 0.0\n",
    "        \n",
    "        # Game progress (normalized)\n",
    "        game_progress = (13 - remaining_turns) / 13.0\n",
    "        \n",
    "        # Upper section bonus situation\n",
    "        upper_score = 0\n",
    "        for i in range(6):\n",
    "            if categories_available[i] == 0:  # Category already filled\n",
    "                # Try to extract the score from the environment if available\n",
    "                if hasattr(self.env, 'scorecard'):\n",
    "                    upper_score += self.env.scorecard.get(self.categories[i], 0)\n",
    "        \n",
    "        upper_bonus_threshold = 63\n",
    "        upper_bonus_progress = min(1.0, upper_score / upper_bonus_threshold)\n",
    "        \n",
    "        # Estimated potential to reach upper bonus\n",
    "        remaining_upper_potential = 0\n",
    "        for i in range(6):\n",
    "            if categories_available[i] == 1:\n",
    "                # Use average expected value for each category\n",
    "                remaining_upper_potential += min((i+1) * 3, (i+1) * 5)  # Conservative estimate\n",
    "        \n",
    "        upper_bonus_potential = min(1.0, (upper_score + remaining_upper_potential) / upper_bonus_threshold)\n",
    "        \n",
    "        # Combine original observation with derived features\n",
    "        enhanced = np.concatenate([\n",
    "            observation,\n",
    "            dice_counts,\n",
    "            upper_potentials,\n",
    "            [has_three_kind, has_four_kind, has_yahtzee, has_pair],\n",
    "            [small_straight_potential, large_straight_potential, full_house_potential],\n",
    "            [game_progress, upper_bonus_progress, upper_bonus_potential]\n",
    "        ])\n",
    "        \n",
    "        return enhanced.astype(np.float32)\n",
    "\n",
    "    def select_target_category(self, observation, is_eval=False):\n",
    "        \"\"\"Select a target category based on the current observation\"\"\"\n",
    "        enhanced_obs = self.enhance_observation(observation)\n",
    "        state = torch.from_numpy(enhanced_obs).float().to(self.device)\n",
    "        \n",
    "        # Get available categories\n",
    "        categories_available = observation[5:18]\n",
    "        legal_categories = [i for i in range(13) if categories_available[i] == 1]\n",
    "        \n",
    "        if not legal_categories:\n",
    "            return None  # No legal categories left\n",
    "        \n",
    "        # Use epsilon-greedy for exploration during training\n",
    "        if not is_eval and random.random() < self.epsilon:\n",
    "            return random.choice(legal_categories)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get Q-values for all categories\n",
    "            q_values = self.target_intuition_net(state.unsqueeze(0)).squeeze(0)\n",
    "            \n",
    "            # Mask unavailable categories with large negative values\n",
    "            mask = torch.ones(13, device=self.device) * -1000000\n",
    "            for i in legal_categories:\n",
    "                mask[i] = 0\n",
    "            masked_q = q_values + mask\n",
    "            \n",
    "            # Select category with highest Q-value\n",
    "            return torch.argmax(masked_q).item()\n",
    "\n",
    "    def calculate_reroll_strategy(self, dice, target_category, rolls_remaining):\n",
    "        \n",
    "        return self.reroller.calculate_reroll_strategy(dice, self.categories[target_category], rolls_remaining)\n",
    "\n",
    "    def select_action(self, observation, is_eval=False, target_category=None):\n",
    "        \"\"\"\n",
    "        Select an action based on the current observation\n",
    "        Returns: (is_category_selection, action)\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        dice = observation[:5]\n",
    "        categories_available = observation[5:18]\n",
    "        rolls_remaining = observation[18]\n",
    "        \n",
    "        # If no target provided, compute one\n",
    "        if target_category is None:\n",
    "            target_category = self.select_target_category(observation, is_eval)\n",
    "        \n",
    "        # When no rerolls are left or no categories are available for targeting,\n",
    "        # we must select a category to play\n",
    "        if rolls_remaining == 0 or all(categories_available[i] == 0 for i in range(13)):\n",
    "            is_category_selection = True\n",
    "            action = self.select_target_category(observation, is_eval)\n",
    "        else:\n",
    "            # Otherwise, calculate reroll pattern\n",
    "            is_category_selection = False\n",
    "            decisions = self.calculate_reroll_strategy(dice, target_category, rolls_remaining)\n",
    "            action = int(''.join(map(str, decisions)), 2) + 13\n",
    "        \n",
    "        # Verify action is legal\n",
    "        legal_actions = self.env.get_legal_actions()\n",
    "        if action >= len(legal_actions) or legal_actions[action] != 1:\n",
    "            # Fallback to a legal action\n",
    "            legal_indices = [i for i, is_legal in enumerate(legal_actions) if is_legal == 1]\n",
    "            if legal_indices:\n",
    "                if action < 13:  # Category selection\n",
    "                    is_category_selection = True\n",
    "                    action = random.choice([i for i in legal_indices if i < 13])\n",
    "                else:  # Reroll action\n",
    "                    is_category_selection = False\n",
    "                    action = random.choice([i for i in legal_indices if i >= 13])\n",
    "        \n",
    "        end = time.time()\n",
    "        self.times['select_action'].append(end - start)\n",
    "        return is_category_selection, action, target_category\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Train the target intuition network from experiences\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from the replay buffer\n",
    "        samples = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Filter for category selection actions (0-12) only\n",
    "        category_mask = actions < 13\n",
    "        if category_mask.any():\n",
    "            category_states = states[category_mask]\n",
    "            category_actions = actions[category_mask]\n",
    "            category_rewards = rewards[category_mask]\n",
    "            category_next_states = next_states[category_mask]\n",
    "            category_dones = dones[category_mask]\n",
    "            \n",
    "            # Calculate current Q-values\n",
    "            current_q = self.target_intuition_net(category_states).gather(1, category_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Calculate target Q-values (Double DQN approach)\n",
    "            with torch.no_grad():\n",
    "                next_q_values = self.target_intuition_net(category_next_states)\n",
    "                next_actions = next_q_values.max(1)[1].unsqueeze(1)\n",
    "                next_q = next_q_values.gather(1, next_actions).squeeze(1)\n",
    "                target_q = category_rewards + (1 - category_dones) * self.gamma * next_q\n",
    "            \n",
    "            # Calculate loss and optimize\n",
    "            loss = F.smooth_l1_loss(current_q, target_q)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.target_intuition_net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            end = time.time()\n",
    "            self.times['optimize_model'].append(end - start)\n",
    "            return loss.item()\n",
    "        \n",
    "        end = time.time()\n",
    "        self.times['optimize_model'].append(end - start)\n",
    "        return None\n",
    "\n",
    "    def shape_reward(self, reward, action, observation, next_observation, done, info):\n",
    "        \"\"\"Apply reward shaping to encourage better strategic play\"\"\"\n",
    "        shaped_reward = reward\n",
    "        dice = observation[:5] + 1  # Convert 0-5 to 1-6\n",
    "        categories_available = observation[5:18]\n",
    "        \n",
    "        # Category selection (actions 0-12)\n",
    "        if action < 13:\n",
    "            # Track the chosen category\n",
    "            category = self.categories[action]\n",
    "            if reward == 0:\n",
    "                remaining_categories = sum(categories_available)\n",
    "                if remaining_categories <= 3:\n",
    "                    shaped_reward = -2  # Less penalty for strategic zeros late game\n",
    "                else:\n",
    "                    shaped_reward = -5  # Standard penalty\n",
    "            \n",
    "            # Upper section scoring\n",
    "            if action < 6:\n",
    "                # Calculate current upper section total\n",
    "                upper_total = 0\n",
    "                if hasattr(self.env, 'scorecard'):\n",
    "                    for i in range(6):\n",
    "                        if i != action and not categories_available[i]:  # Category already filled\n",
    "                            upper_total += self.env.scorecard.get(self.categories[i], 0)\n",
    "                \n",
    "                # Add current category score\n",
    "                upper_with_current = upper_total + reward\n",
    "\n",
    "                if reward >= (action + 1) * 3:\n",
    "                    shaped_reward += 15  # Reward good upper section scores\n",
    "            \n",
    "            # Reward efficient use of categories\n",
    "            if category == 'yahtzee' and reward >= 50:\n",
    "                shaped_reward += 30  # Extra bonus for Yahtzee\n",
    "            elif category in ['small_straight', 'large_straight'] and reward > 0:\n",
    "                shaped_reward += 25  # Bonus for straights\n",
    "            elif category == 'full_house' and reward > 0:\n",
    "                shaped_reward += 25  # Bonus for full house\n",
    "            elif category in ['three_of_a_kind', 'four_of_a_kind'] and reward >= (3 if category == 'three_of_a_kind' else 4) * 4:\n",
    "                shaped_reward += 10  # Bonus for good three/four of a kind\n",
    "        \n",
    "        return shaped_reward\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "    def evaluate(self, num_episodes=10):\n",
    "        \"\"\"Evaluate the agent's performance without exploration\"\"\"\n",
    "        total_scores = []\n",
    "        upper_bonus_count = 0\n",
    "        category_usage = {cat: 0 for cat in self.categories}\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            observation, _ = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            # Initial target category selection\n",
    "            target_category = self.select_target_category(observation, is_eval=True)\n",
    "            \n",
    "            while not done:\n",
    "                # Select action based on target category\n",
    "                is_category_selection, action, target_category = self.select_action(\n",
    "                    observation, is_eval=True, target_category=target_category\n",
    "                )\n",
    "                \n",
    "                # Track category usage\n",
    "                if is_category_selection:\n",
    "                    category_usage[self.categories[action]] += 1\n",
    "                \n",
    "                # Take action\n",
    "                observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Recalculate target after each step if there are rerolls left\n",
    "                if not done and observation[18] > 0:  # rolls_remaining > 0\n",
    "                    target_category = self.select_target_category(observation, is_eval=True)\n",
    "            \n",
    "            total_scores.append(episode_reward)\n",
    "            \n",
    "            # Check if upper bonus was achieved\n",
    "            if hasattr(self.env, 'scorecard') and 'upper_bonus' in self.env.scorecard:\n",
    "                if self.env.scorecard['upper_bonus'] > 0:\n",
    "                    upper_bonus_count += 1\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['scores'].extend(total_scores)\n",
    "        self.stats['upper_bonus_achieved'] += upper_bonus_count\n",
    "        self.stats['total_games'] += num_episodes\n",
    "        \n",
    "        # Merge category usage\n",
    "        for cat, count in category_usage.items():\n",
    "            self.stats['category_usage'][cat] += count\n",
    "            \n",
    "        return np.mean(total_scores)\n",
    "class YahtzeeTrainerWithCurriculum:\n",
    "    def __init__(self, env, device):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.agent = YahtzeeAgent(self.env, self.device)\n",
    "        \n",
    "        # Curriculum learning parameters\n",
    "        self.curriculum_stage = 0\n",
    "        self.curriculum_stages = [\n",
    "            {\"name\": \"Upper Section Focus\", \"episodes\": 2500},\n",
    "            {\"name\": \"Basic Combinations\", \"episodes\": 5000},\n",
    "            {\"name\": \"Full Game\", \"episodes\": 9000},\n",
    "        ]\n",
    "        \n",
    "        # Override agent's reward shaping based on curriculum\n",
    "        self.original_shape_reward = self.agent.shape_reward\n",
    "        self.agent.shape_reward = self.curriculum_shape_reward\n",
    "        \n",
    "        # Original select_target method\n",
    "        self.original_select_target = self.agent.select_target_category\n",
    "        \n",
    "        # Override target selection based on curriculum\n",
    "        self.agent.select_target_category = self.curriculum_select_target\n",
    "    \n",
    "    def get_curriculum_stage(self, episode):\n",
    "        \"\"\"Determine the current curriculum stage based on episode number\"\"\"\n",
    "        completed_episodes = 0\n",
    "        for i, stage in enumerate(self.curriculum_stages):\n",
    "            completed_episodes += stage[\"episodes\"]\n",
    "            if episode < completed_episodes or i == len(self.curriculum_stages) - 1:\n",
    "                return i\n",
    "        return len(self.curriculum_stages) - 1\n",
    "    \n",
    "    def curriculum_select_target(self, observation, is_eval=False):\n",
    "        \"\"\"Improved target selection using Q-values within curriculum constraints\"\"\"\n",
    "        # Use original method for evaluation\n",
    "        if is_eval:\n",
    "            return self.original_select_target(observation, is_eval=True)\n",
    "            \n",
    "        categories_available = observation[5:18]\n",
    "        legal_categories = [i for i in range(13) if categories_available[i] == 1]\n",
    "        \n",
    "        if not legal_categories:\n",
    "            return None\n",
    "    \n",
    "        # Prepare state\n",
    "        enhanced_obs = self.agent.enhance_observation(observation)\n",
    "        state = torch.from_numpy(enhanced_obs).float().to(self.agent.device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            q_values = self.agent.target_intuition_net(state.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "        # Stage 0: Focus on upper section\n",
    "        if self.curriculum_stage == 0:\n",
    "            upper_categories = [i for i in range(6) if i in legal_categories]\n",
    "            if upper_categories and random.random() < 0.8:\n",
    "                # Choose best among upper section\n",
    "                best_upper = max(upper_categories, key=lambda i: q_values[i].item())\n",
    "                return best_upper\n",
    "    \n",
    "        # Stage 1: Basic combinations\n",
    "        elif self.curriculum_stage == 1:\n",
    "            upper_categories = [i for i in range(6) if i in legal_categories]\n",
    "            if upper_categories and random.random() < 0.5:\n",
    "                best_upper = max(upper_categories, key=lambda i: q_values[i].item())\n",
    "                return best_upper\n",
    "            \n",
    "            basic_lower = [i for i in [6, 7, 12] if i in legal_categories]\n",
    "            if basic_lower and random.random() < 0.3:\n",
    "                best_basic = max(basic_lower, key=lambda i: q_values[i].item())\n",
    "                return best_basic\n",
    "    \n",
    "        # Stage 2 or fallback: Use full legal category set\n",
    "        epsilon_scale = 1.0\n",
    "        if random.random() < (self.agent.epsilon * epsilon_scale):\n",
    "            return random.choice(legal_categories)\n",
    "        \n",
    "        # Mask unavailable categories for final selection\n",
    "        mask = torch.ones(13, device=self.agent.device) * -1000000\n",
    "        for i in legal_categories:\n",
    "            mask[i] = 0\n",
    "        masked_q = q_values + mask\n",
    "        return torch.argmax(masked_q).item()\n",
    "\n",
    "    \n",
    "    def curriculum_shape_reward(self, reward, action, observation, next_observation, done, info):\n",
    "        \n",
    "        return self.original_shape_reward(reward, action, observation, next_observation, done, info)\n",
    "        \n",
    "    def train(self, num_episodes=50000, eval_freq=100):\n",
    "        \"\"\"\n",
    "        Train the agent with curriculum learning over a specified number of episodes\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        eval_scores = []\n",
    "        losses = []\n",
    "        curriculum_transitions = []\n",
    "        \n",
    "        # Performance tracking\n",
    "        best_eval_score = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "\n",
    "            if episode == 17500:\n",
    "                for param_group in self.agent.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.00001\n",
    "\n",
    "            if episode == 30000:\n",
    "                for param_group in self.agent.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.000005\n",
    "                        \n",
    "            # Check if we need to update curriculum stage\n",
    "            curr_stage = self.get_curriculum_stage(episode)\n",
    "            if curr_stage != self.curriculum_stage:\n",
    "                # Transition to new curriculum stage\n",
    "                old_stage = self.curriculum_stage\n",
    "                self.curriculum_stage = curr_stage\n",
    "                curriculum_transitions.append((episode, \n",
    "                                             self.curriculum_stages[old_stage][\"name\"],\n",
    "                                             self.curriculum_stages[curr_stage][\"name\"]))\n",
    "                print(f\"\\nAdvancing curriculum: {self.curriculum_stages[old_stage]['name']} -> {self.curriculum_stages[curr_stage]['name']} at episode {episode}\\n\")\n",
    "            \n",
    "            observation, _ = self.agent.env.reset()\n",
    "            total_reward = 0\n",
    "            episode_losses = []\n",
    "            done = False\n",
    "            \n",
    "            # Enhanced observation for the buffer\n",
    "            enhanced_obs = self.agent.enhance_observation(observation)\n",
    "            \n",
    "            # Initial target category selection using curriculum method\n",
    "            target_category = self.agent.select_target_category(observation)\n",
    "            initial_target = target_category\n",
    "            \n",
    "            # Track curriculum statistics\n",
    "            curriculum_stats = {\n",
    "                \"stage\": self.curriculum_stage,\n",
    "                \"upper_section_used\": 0,\n",
    "                \"lower_section_used\": 0\n",
    "            }\n",
    "            \n",
    "            # Safety limit to prevent infinite loops\n",
    "            step_count = 0\n",
    "            max_steps = 100\n",
    "            \n",
    "            while not done and step_count < max_steps:\n",
    "                # Select action based on the current target category\n",
    "                is_category_selection, action, target_category = self.agent.select_action(\n",
    "                    observation, target_category=target_category\n",
    "                )\n",
    "                \n",
    "                # Take the action\n",
    "                new_observation, reward, terminated, truncated, info = self.agent.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Track curriculum-specific statistics\n",
    "                if is_category_selection:\n",
    "                    if action < 6:  # Upper section\n",
    "                        curriculum_stats[\"upper_section_used\"] += 1\n",
    "                    else:  # Lower section\n",
    "                        curriculum_stats[\"lower_section_used\"] += 1\n",
    "                \n",
    "                # Enhanced observation for the buffer\n",
    "                enhanced_new_obs = self.agent.enhance_observation(new_observation)\n",
    "                \n",
    "                # Apply curriculum-specific reward shaping\n",
    "                shaped_reward = self.agent.shape_reward(reward, action, observation, new_observation, done, info)\n",
    "                total_reward += reward  # Track original reward for reporting\n",
    "                \n",
    "                # If this was a category selection, store it in replay buffer\n",
    "                if is_category_selection:\n",
    "                    # Store transition in replay buffer\n",
    "                    self.agent.buffer.append((enhanced_obs, action, shaped_reward, enhanced_new_obs, done))\n",
    "                    \n",
    "                    # Update target category statistics\n",
    "                    if target_category == action:\n",
    "                        self.agent.stats['final_category_matches_target'] += 1\n",
    "                    \n",
    "                    # Train the network\n",
    "                    loss = self.agent.optimize_model()\n",
    "                    if loss is not None:\n",
    "                        episode_losses.append(loss)\n",
    "                elif not done:\n",
    "                    # After reroll, recalculate target category based on new observation\n",
    "                    new_target = self.agent.select_target_category(new_observation)\n",
    "                    \n",
    "                    # Track target changes\n",
    "                    if new_target != target_category:\n",
    "                        self.agent.stats['target_switches'] += 1\n",
    "                    \n",
    "                    target_category = new_target\n",
    "                \n",
    "                # Update for next step\n",
    "                observation = new_observation\n",
    "                enhanced_obs = enhanced_new_obs\n",
    "                step_count += 1\n",
    "            \n",
    "            decay_factor = 0.99995\n",
    "            self.agent.epsilon = max(self.agent.epsilon_min, self.agent.epsilon * decay_factor)\n",
    "            \n",
    "            # Record episode reward\n",
    "            rewards.append(total_reward)\n",
    "            \n",
    "            # Record mean loss if available\n",
    "            if episode_losses:\n",
    "                losses.append(np.mean(episode_losses))\n",
    "            \n",
    "            # Evaluate periodically\n",
    "            if (episode + 1) % eval_freq == 0:\n",
    "                eval_score = self.agent.evaluate(num_episodes=50)\n",
    "                eval_scores.append(eval_score)\n",
    "                \n",
    "                # Print progress with curriculum info\n",
    "                print(f\"Episode {episode+1}/{num_episodes}, Stage: {self.curriculum_stages[self.curriculum_stage]['name']}\")\n",
    "                print(f\"Avg. Reward: {np.mean(rewards[-eval_freq:]):.1f}, Eval Score: {eval_score:.1f}, Epsilon: {self.agent.epsilon:.3f}\")\n",
    "                \n",
    "                # Save best model\n",
    "                if eval_score > best_eval_score:\n",
    "                    best_eval_score = eval_score\n",
    "                    best_model_state = {\n",
    "                        'target_intuition_net': self.agent.target_intuition_net.state_dict(),\n",
    "                        'optimizer': self.agent.optimizer.state_dict(),\n",
    "                        'epsilon': self.agent.epsilon,\n",
    "                        'curriculum_stage': self.curriculum_stage\n",
    "                    }\n",
    "\n",
    "                print()\n",
    "      \n",
    "        # Save the best model\n",
    "        if best_model_state:\n",
    "            torch.save(best_model_state, \"best_yahtzee_curriculum_model.pt\")\n",
    "\n",
    "        final_model_state = {\n",
    "            'target_intuition_net': self.agent.target_intuition_net.state_dict(),\n",
    "            'optimizer': self.agent.optimizer.state_dict(),\n",
    "            'epsilon': self.agent.epsilon,\n",
    "            'curriculum_stage': self.curriculum_stage\n",
    "        }\n",
    "        torch.save(final_model_state, \"final_curriculum_model.pt\")\n",
    "        \n",
    "        # Return training history with curriculum information\n",
    "        return {\n",
    "            'rewards': rewards,\n",
    "            'eval_scores': eval_scores,\n",
    "            'losses': losses,\n",
    "            'stats': self.agent.stats,\n",
    "            'curriculum_transitions': curriculum_transitions\n",
    "        }\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load a trained model from a file (unchanged)\"\"\"\n",
    "        # This method is unchanged\n",
    "        checkpoint = torch.load(path)\n",
    "        self.agent.target_intuition_net.load_state_dict(checkpoint['target_intuition_net'])\n",
    "        self.agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.agent.epsilon = checkpoint['epsilon']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        \n",
    "        # Also load curriculum stage if available\n",
    "        checkpoint = torch.load(path)\n",
    "        if 'curriculum_stage' in checkpoint:\n",
    "            self.curriculum_stage = checkpoint['curriculum_stage']\n",
    "            print(f\"Loaded curriculum stage: {self.curriculum_stages[self.curriculum_stage]['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6795db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:23:43.697350Z",
     "iopub.status.busy": "2025-04-07T08:23:43.697035Z",
     "iopub.status.idle": "2025-04-07T09:55:39.259131Z",
     "shell.execute_reply": "2025-04-07T09:55:39.258215Z"
    },
    "papermill": {
     "duration": 5515.566872,
     "end_time": "2025-04-07T09:55:39.260633",
     "exception": false,
     "start_time": "2025-04-07T08:23:43.693761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 74.8, Eval Score: 114.7, Epsilon: 1.200\n",
      "\n",
      "Episode 200/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 81.8, Eval Score: 140.5, Epsilon: 1.200\n",
      "\n",
      "Episode 300/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 82.2, Eval Score: 139.9, Epsilon: 1.200\n",
      "\n",
      "Episode 400/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 89.9, Eval Score: 143.0, Epsilon: 1.200\n",
      "\n",
      "Episode 500/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 87.2, Eval Score: 147.5, Epsilon: 1.200\n",
      "\n",
      "Episode 600/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 86.7, Eval Score: 150.8, Epsilon: 1.200\n",
      "\n",
      "Episode 700/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 89.9, Eval Score: 159.4, Epsilon: 1.200\n",
      "\n",
      "Episode 800/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 87.0, Eval Score: 152.9, Epsilon: 1.200\n",
      "\n",
      "Episode 900/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 82.1, Eval Score: 148.7, Epsilon: 1.200\n",
      "\n",
      "Episode 1000/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 85.0, Eval Score: 147.1, Epsilon: 1.200\n",
      "\n",
      "Episode 1100/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 80.4, Eval Score: 142.0, Epsilon: 1.200\n",
      "\n",
      "Episode 1200/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 88.1, Eval Score: 148.2, Epsilon: 1.200\n",
      "\n",
      "Episode 1300/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 89.1, Eval Score: 141.9, Epsilon: 1.200\n",
      "\n",
      "Episode 1400/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 88.0, Eval Score: 159.5, Epsilon: 1.200\n",
      "\n",
      "Episode 1500/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 83.1, Eval Score: 149.6, Epsilon: 1.200\n",
      "\n",
      "Episode 1600/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 82.9, Eval Score: 156.6, Epsilon: 1.200\n",
      "\n",
      "Episode 1700/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 85.2, Eval Score: 159.6, Epsilon: 1.200\n",
      "\n",
      "Episode 1800/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 85.1, Eval Score: 156.4, Epsilon: 1.200\n",
      "\n",
      "Episode 1900/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 86.3, Eval Score: 153.7, Epsilon: 1.200\n",
      "\n",
      "Episode 2000/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 87.3, Eval Score: 159.8, Epsilon: 1.200\n",
      "\n",
      "Episode 2100/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 86.6, Eval Score: 161.2, Epsilon: 1.200\n",
      "\n",
      "Episode 2200/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 85.5, Eval Score: 159.6, Epsilon: 1.200\n",
      "\n",
      "Episode 2300/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 85.4, Eval Score: 160.9, Epsilon: 1.200\n",
      "\n",
      "Episode 2400/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 86.3, Eval Score: 161.4, Epsilon: 1.200\n",
      "\n",
      "Episode 2500/50000, Stage: Upper Section Focus\n",
      "Avg. Reward: 86.4, Eval Score: 168.7, Epsilon: 1.200\n",
      "\n",
      "\n",
      "Advancing curriculum: Upper Section Focus -> Basic Combinations at episode 2500\n",
      "\n",
      "Episode 2600/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 85.0, Eval Score: 165.9, Epsilon: 1.200\n",
      "\n",
      "Episode 2700/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 80.8, Eval Score: 159.9, Epsilon: 1.200\n",
      "\n",
      "Episode 2800/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 85.4, Eval Score: 168.7, Epsilon: 1.200\n",
      "\n",
      "Episode 2900/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 81.3, Eval Score: 169.9, Epsilon: 1.200\n",
      "\n",
      "Episode 3000/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 81.9, Eval Score: 169.3, Epsilon: 1.200\n",
      "\n",
      "Episode 3100/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.0, Eval Score: 164.4, Epsilon: 1.200\n",
      "\n",
      "Episode 3200/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 83.7, Eval Score: 177.2, Epsilon: 1.200\n",
      "\n",
      "Episode 3300/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 85.3, Eval Score: 176.8, Epsilon: 1.200\n",
      "\n",
      "Episode 3400/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 83.7, Eval Score: 173.3, Epsilon: 1.200\n",
      "\n",
      "Episode 3500/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 84.3, Eval Score: 176.8, Epsilon: 1.200\n",
      "\n",
      "Episode 3600/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 84.8, Eval Score: 175.3, Epsilon: 1.200\n",
      "\n",
      "Episode 3700/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 90.4, Eval Score: 180.7, Epsilon: 1.200\n",
      "\n",
      "Episode 3800/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 78.6, Eval Score: 176.3, Epsilon: 1.200\n",
      "\n",
      "Episode 3900/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 82.4, Eval Score: 182.1, Epsilon: 1.200\n",
      "\n",
      "Episode 4000/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 85.5, Eval Score: 180.3, Epsilon: 1.200\n",
      "\n",
      "Episode 4100/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 87.9, Eval Score: 178.5, Epsilon: 1.200\n",
      "\n",
      "Episode 4200/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 88.2, Eval Score: 184.6, Epsilon: 1.200\n",
      "\n",
      "Episode 4300/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 85.4, Eval Score: 178.1, Epsilon: 1.200\n",
      "\n",
      "Episode 4400/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.2, Eval Score: 193.7, Epsilon: 1.200\n",
      "\n",
      "Episode 4500/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.8, Eval Score: 181.4, Epsilon: 1.200\n",
      "\n",
      "Episode 4600/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 88.3, Eval Score: 195.3, Epsilon: 1.200\n",
      "\n",
      "Episode 4700/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 87.0, Eval Score: 176.9, Epsilon: 1.200\n",
      "\n",
      "Episode 4800/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 88.3, Eval Score: 178.5, Epsilon: 1.200\n",
      "\n",
      "Episode 4900/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.5, Eval Score: 179.6, Epsilon: 1.200\n",
      "\n",
      "Episode 5000/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 87.9, Eval Score: 186.9, Epsilon: 1.200\n",
      "\n",
      "Episode 5100/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.3, Eval Score: 184.8, Epsilon: 1.200\n",
      "\n",
      "Episode 5200/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 85.9, Eval Score: 187.7, Epsilon: 1.200\n",
      "\n",
      "Episode 5300/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 84.8, Eval Score: 186.4, Epsilon: 1.200\n",
      "\n",
      "Episode 5400/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 88.2, Eval Score: 185.2, Epsilon: 1.200\n",
      "\n",
      "Episode 5500/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 89.3, Eval Score: 185.4, Epsilon: 1.200\n",
      "\n",
      "Episode 5600/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.4, Eval Score: 184.7, Epsilon: 1.200\n",
      "\n",
      "Episode 5700/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.5, Eval Score: 184.7, Epsilon: 1.200\n",
      "\n",
      "Episode 5800/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 84.5, Eval Score: 186.9, Epsilon: 1.200\n",
      "\n",
      "Episode 5900/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 84.4, Eval Score: 177.2, Epsilon: 1.200\n",
      "\n",
      "Episode 6000/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.0, Eval Score: 191.0, Epsilon: 1.200\n",
      "\n",
      "Episode 6100/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 82.4, Eval Score: 186.5, Epsilon: 1.200\n",
      "\n",
      "Episode 6200/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 90.5, Eval Score: 185.0, Epsilon: 1.200\n",
      "\n",
      "Episode 6300/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 83.0, Eval Score: 182.0, Epsilon: 1.200\n",
      "\n",
      "Episode 6400/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 84.3, Eval Score: 187.1, Epsilon: 1.200\n",
      "\n",
      "Episode 6500/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 79.3, Eval Score: 179.6, Epsilon: 1.200\n",
      "\n",
      "Episode 6600/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 82.4, Eval Score: 201.3, Epsilon: 1.200\n",
      "\n",
      "Episode 6700/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 83.7, Eval Score: 181.3, Epsilon: 1.200\n",
      "\n",
      "Episode 6800/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 83.0, Eval Score: 187.2, Epsilon: 1.200\n",
      "\n",
      "Episode 6900/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.7, Eval Score: 195.0, Epsilon: 1.200\n",
      "\n",
      "Episode 7000/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 83.0, Eval Score: 184.9, Epsilon: 1.200\n",
      "\n",
      "Episode 7100/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 86.2, Eval Score: 192.3, Epsilon: 1.200\n",
      "\n",
      "Episode 7200/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 87.8, Eval Score: 193.3, Epsilon: 1.200\n",
      "\n",
      "Episode 7300/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 80.3, Eval Score: 186.1, Epsilon: 1.200\n",
      "\n",
      "Episode 7400/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 81.5, Eval Score: 196.4, Epsilon: 1.200\n",
      "\n",
      "Episode 7500/50000, Stage: Basic Combinations\n",
      "Avg. Reward: 80.3, Eval Score: 190.7, Epsilon: 1.200\n",
      "\n",
      "\n",
      "Advancing curriculum: Basic Combinations -> Full Game at episode 7500\n",
      "\n",
      "Episode 7600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 199.6, Epsilon: 1.200\n",
      "\n",
      "Episode 7700/50000, Stage: Full Game\n",
      "Avg. Reward: 61.3, Eval Score: 192.3, Epsilon: 1.200\n",
      "\n",
      "Episode 7800/50000, Stage: Full Game\n",
      "Avg. Reward: 57.4, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 7900/50000, Stage: Full Game\n",
      "Avg. Reward: 63.3, Eval Score: 193.6, Epsilon: 1.200\n",
      "\n",
      "Episode 8000/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 190.8, Epsilon: 1.200\n",
      "\n",
      "Episode 8100/50000, Stage: Full Game\n",
      "Avg. Reward: 64.8, Eval Score: 188.8, Epsilon: 1.200\n",
      "\n",
      "Episode 8200/50000, Stage: Full Game\n",
      "Avg. Reward: 60.0, Eval Score: 179.6, Epsilon: 1.200\n",
      "\n",
      "Episode 8300/50000, Stage: Full Game\n",
      "Avg. Reward: 60.1, Eval Score: 188.8, Epsilon: 1.200\n",
      "\n",
      "Episode 8400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.3, Eval Score: 195.2, Epsilon: 1.200\n",
      "\n",
      "Episode 8500/50000, Stage: Full Game\n",
      "Avg. Reward: 63.8, Eval Score: 190.3, Epsilon: 1.200\n",
      "\n",
      "Episode 8600/50000, Stage: Full Game\n",
      "Avg. Reward: 58.8, Eval Score: 192.2, Epsilon: 1.200\n",
      "\n",
      "Episode 8700/50000, Stage: Full Game\n",
      "Avg. Reward: 57.9, Eval Score: 194.6, Epsilon: 1.200\n",
      "\n",
      "Episode 8800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.8, Eval Score: 196.9, Epsilon: 1.200\n",
      "\n",
      "Episode 8900/50000, Stage: Full Game\n",
      "Avg. Reward: 67.8, Eval Score: 194.4, Epsilon: 1.200\n",
      "\n",
      "Episode 9000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.0, Eval Score: 199.5, Epsilon: 1.200\n",
      "\n",
      "Episode 9100/50000, Stage: Full Game\n",
      "Avg. Reward: 62.1, Eval Score: 202.5, Epsilon: 1.200\n",
      "\n",
      "Episode 9200/50000, Stage: Full Game\n",
      "Avg. Reward: 63.1, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 9300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 207.8, Epsilon: 1.200\n",
      "\n",
      "Episode 9400/50000, Stage: Full Game\n",
      "Avg. Reward: 64.8, Eval Score: 191.3, Epsilon: 1.200\n",
      "\n",
      "Episode 9500/50000, Stage: Full Game\n",
      "Avg. Reward: 64.6, Eval Score: 195.7, Epsilon: 1.200\n",
      "\n",
      "Episode 9600/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 201.8, Epsilon: 1.200\n",
      "\n",
      "Episode 9700/50000, Stage: Full Game\n",
      "Avg. Reward: 58.8, Eval Score: 210.0, Epsilon: 1.200\n",
      "\n",
      "Episode 9800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.5, Eval Score: 198.8, Epsilon: 1.200\n",
      "\n",
      "Episode 9900/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 198.4, Epsilon: 1.200\n",
      "\n",
      "Episode 10000/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 199.7, Epsilon: 1.200\n",
      "\n",
      "Episode 10100/50000, Stage: Full Game\n",
      "Avg. Reward: 60.1, Eval Score: 205.5, Epsilon: 1.200\n",
      "\n",
      "Episode 10200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.7, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 10300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.3, Eval Score: 193.5, Epsilon: 1.200\n",
      "\n",
      "Episode 10400/50000, Stage: Full Game\n",
      "Avg. Reward: 59.0, Eval Score: 193.3, Epsilon: 1.200\n",
      "\n",
      "Episode 10500/50000, Stage: Full Game\n",
      "Avg. Reward: 59.5, Eval Score: 194.9, Epsilon: 1.200\n",
      "\n",
      "Episode 10600/50000, Stage: Full Game\n",
      "Avg. Reward: 57.4, Eval Score: 210.1, Epsilon: 1.200\n",
      "\n",
      "Episode 10700/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 199.5, Epsilon: 1.200\n",
      "\n",
      "Episode 10800/50000, Stage: Full Game\n",
      "Avg. Reward: 60.2, Eval Score: 196.2, Epsilon: 1.200\n",
      "\n",
      "Episode 10900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.2, Eval Score: 199.9, Epsilon: 1.200\n",
      "\n",
      "Episode 11000/50000, Stage: Full Game\n",
      "Avg. Reward: 57.6, Eval Score: 194.8, Epsilon: 1.200\n",
      "\n",
      "Episode 11100/50000, Stage: Full Game\n",
      "Avg. Reward: 64.6, Eval Score: 192.6, Epsilon: 1.200\n",
      "\n",
      "Episode 11200/50000, Stage: Full Game\n",
      "Avg. Reward: 62.1, Eval Score: 204.3, Epsilon: 1.200\n",
      "\n",
      "Episode 11300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 208.2, Epsilon: 1.200\n",
      "\n",
      "Episode 11400/50000, Stage: Full Game\n",
      "Avg. Reward: 59.2, Eval Score: 196.6, Epsilon: 1.200\n",
      "\n",
      "Episode 11500/50000, Stage: Full Game\n",
      "Avg. Reward: 57.0, Eval Score: 203.7, Epsilon: 1.200\n",
      "\n",
      "Episode 11600/50000, Stage: Full Game\n",
      "Avg. Reward: 63.8, Eval Score: 200.5, Epsilon: 1.200\n",
      "\n",
      "Episode 11700/50000, Stage: Full Game\n",
      "Avg. Reward: 59.8, Eval Score: 188.1, Epsilon: 1.200\n",
      "\n",
      "Episode 11800/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 206.5, Epsilon: 1.200\n",
      "\n",
      "Episode 11900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.9, Eval Score: 205.6, Epsilon: 1.200\n",
      "\n",
      "Episode 12000/50000, Stage: Full Game\n",
      "Avg. Reward: 62.3, Eval Score: 203.7, Epsilon: 1.200\n",
      "\n",
      "Episode 12100/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 196.2, Epsilon: 1.200\n",
      "\n",
      "Episode 12200/50000, Stage: Full Game\n",
      "Avg. Reward: 59.8, Eval Score: 196.6, Epsilon: 1.200\n",
      "\n",
      "Episode 12300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 205.6, Epsilon: 1.200\n",
      "\n",
      "Episode 12400/50000, Stage: Full Game\n",
      "Avg. Reward: 63.3, Eval Score: 201.9, Epsilon: 1.200\n",
      "\n",
      "Episode 12500/50000, Stage: Full Game\n",
      "Avg. Reward: 63.5, Eval Score: 199.0, Epsilon: 1.200\n",
      "\n",
      "Episode 12600/50000, Stage: Full Game\n",
      "Avg. Reward: 59.7, Eval Score: 190.4, Epsilon: 1.200\n",
      "\n",
      "Episode 12700/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 197.7, Epsilon: 1.200\n",
      "\n",
      "Episode 12800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.2, Eval Score: 197.0, Epsilon: 1.200\n",
      "\n",
      "Episode 12900/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 193.0, Epsilon: 1.200\n",
      "\n",
      "Episode 13000/50000, Stage: Full Game\n",
      "Avg. Reward: 63.1, Eval Score: 199.7, Epsilon: 1.200\n",
      "\n",
      "Episode 13100/50000, Stage: Full Game\n",
      "Avg. Reward: 63.0, Eval Score: 192.4, Epsilon: 1.200\n",
      "\n",
      "Episode 13200/50000, Stage: Full Game\n",
      "Avg. Reward: 62.5, Eval Score: 191.8, Epsilon: 1.200\n",
      "\n",
      "Episode 13300/50000, Stage: Full Game\n",
      "Avg. Reward: 56.8, Eval Score: 201.0, Epsilon: 1.200\n",
      "\n",
      "Episode 13400/50000, Stage: Full Game\n",
      "Avg. Reward: 59.2, Eval Score: 195.3, Epsilon: 1.200\n",
      "\n",
      "Episode 13500/50000, Stage: Full Game\n",
      "Avg. Reward: 59.7, Eval Score: 194.3, Epsilon: 1.200\n",
      "\n",
      "Episode 13600/50000, Stage: Full Game\n",
      "Avg. Reward: 59.1, Eval Score: 199.3, Epsilon: 1.200\n",
      "\n",
      "Episode 13700/50000, Stage: Full Game\n",
      "Avg. Reward: 62.5, Eval Score: 198.5, Epsilon: 1.200\n",
      "\n",
      "Episode 13800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.1, Eval Score: 189.5, Epsilon: 1.200\n",
      "\n",
      "Episode 13900/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 195.2, Epsilon: 1.200\n",
      "\n",
      "Episode 14000/50000, Stage: Full Game\n",
      "Avg. Reward: 56.7, Eval Score: 202.1, Epsilon: 1.200\n",
      "\n",
      "Episode 14100/50000, Stage: Full Game\n",
      "Avg. Reward: 59.0, Eval Score: 203.9, Epsilon: 1.200\n",
      "\n",
      "Episode 14200/50000, Stage: Full Game\n",
      "Avg. Reward: 58.5, Eval Score: 190.2, Epsilon: 1.200\n",
      "\n",
      "Episode 14300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.0, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 14400/50000, Stage: Full Game\n",
      "Avg. Reward: 61.4, Eval Score: 203.2, Epsilon: 1.200\n",
      "\n",
      "Episode 14500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 197.0, Epsilon: 1.200\n",
      "\n",
      "Episode 14600/50000, Stage: Full Game\n",
      "Avg. Reward: 61.3, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 14700/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 187.0, Epsilon: 1.200\n",
      "\n",
      "Episode 14800/50000, Stage: Full Game\n",
      "Avg. Reward: 64.6, Eval Score: 191.3, Epsilon: 1.200\n",
      "\n",
      "Episode 14900/50000, Stage: Full Game\n",
      "Avg. Reward: 64.0, Eval Score: 194.6, Epsilon: 1.200\n",
      "\n",
      "Episode 15000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.5, Eval Score: 189.2, Epsilon: 1.200\n",
      "\n",
      "Episode 15100/50000, Stage: Full Game\n",
      "Avg. Reward: 60.9, Eval Score: 193.1, Epsilon: 1.200\n",
      "\n",
      "Episode 15200/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 198.0, Epsilon: 1.200\n",
      "\n",
      "Episode 15300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 198.2, Epsilon: 1.200\n",
      "\n",
      "Episode 15400/50000, Stage: Full Game\n",
      "Avg. Reward: 61.4, Eval Score: 189.9, Epsilon: 1.200\n",
      "\n",
      "Episode 15500/50000, Stage: Full Game\n",
      "Avg. Reward: 59.8, Eval Score: 194.7, Epsilon: 1.200\n",
      "\n",
      "Episode 15600/50000, Stage: Full Game\n",
      "Avg. Reward: 65.8, Eval Score: 201.9, Epsilon: 1.200\n",
      "\n",
      "Episode 15700/50000, Stage: Full Game\n",
      "Avg. Reward: 65.0, Eval Score: 199.7, Epsilon: 1.200\n",
      "\n",
      "Episode 15800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.5, Eval Score: 197.3, Epsilon: 1.200\n",
      "\n",
      "Episode 15900/50000, Stage: Full Game\n",
      "Avg. Reward: 60.2, Eval Score: 207.2, Epsilon: 1.200\n",
      "\n",
      "Episode 16000/50000, Stage: Full Game\n",
      "Avg. Reward: 57.9, Eval Score: 187.0, Epsilon: 1.200\n",
      "\n",
      "Episode 16100/50000, Stage: Full Game\n",
      "Avg. Reward: 59.6, Eval Score: 189.8, Epsilon: 1.200\n",
      "\n",
      "Episode 16200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 189.9, Epsilon: 1.200\n",
      "\n",
      "Episode 16300/50000, Stage: Full Game\n",
      "Avg. Reward: 64.8, Eval Score: 189.5, Epsilon: 1.200\n",
      "\n",
      "Episode 16400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 16500/50000, Stage: Full Game\n",
      "Avg. Reward: 64.6, Eval Score: 196.2, Epsilon: 1.200\n",
      "\n",
      "Episode 16600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.5, Eval Score: 190.1, Epsilon: 1.200\n",
      "\n",
      "Episode 16700/50000, Stage: Full Game\n",
      "Avg. Reward: 60.7, Eval Score: 205.6, Epsilon: 1.200\n",
      "\n",
      "Episode 16800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.2, Eval Score: 201.0, Epsilon: 1.200\n",
      "\n",
      "Episode 16900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.5, Eval Score: 209.6, Epsilon: 1.200\n",
      "\n",
      "Episode 17000/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 198.7, Epsilon: 1.200\n",
      "\n",
      "Episode 17100/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 199.2, Epsilon: 1.200\n",
      "\n",
      "Episode 17200/50000, Stage: Full Game\n",
      "Avg. Reward: 58.7, Eval Score: 204.7, Epsilon: 1.200\n",
      "\n",
      "Episode 17300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.6, Eval Score: 200.3, Epsilon: 1.200\n",
      "\n",
      "Episode 17400/50000, Stage: Full Game\n",
      "Avg. Reward: 61.4, Eval Score: 206.0, Epsilon: 1.200\n",
      "\n",
      "Episode 17500/50000, Stage: Full Game\n",
      "Avg. Reward: 62.9, Eval Score: 196.2, Epsilon: 1.200\n",
      "\n",
      "Episode 17600/50000, Stage: Full Game\n",
      "Avg. Reward: 60.2, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 17700/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 193.8, Epsilon: 1.200\n",
      "\n",
      "Episode 17800/50000, Stage: Full Game\n",
      "Avg. Reward: 67.2, Eval Score: 188.4, Epsilon: 1.200\n",
      "\n",
      "Episode 17900/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 195.5, Epsilon: 1.200\n",
      "\n",
      "Episode 18000/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 194.6, Epsilon: 1.200\n",
      "\n",
      "Episode 18100/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 192.6, Epsilon: 1.200\n",
      "\n",
      "Episode 18200/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 195.6, Epsilon: 1.200\n",
      "\n",
      "Episode 18300/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 197.5, Epsilon: 1.200\n",
      "\n",
      "Episode 18400/50000, Stage: Full Game\n",
      "Avg. Reward: 64.1, Eval Score: 197.6, Epsilon: 1.200\n",
      "\n",
      "Episode 18500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 197.5, Epsilon: 1.200\n",
      "\n",
      "Episode 18600/50000, Stage: Full Game\n",
      "Avg. Reward: 59.2, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 18700/50000, Stage: Full Game\n",
      "Avg. Reward: 58.4, Eval Score: 200.2, Epsilon: 1.200\n",
      "\n",
      "Episode 18800/50000, Stage: Full Game\n",
      "Avg. Reward: 60.6, Eval Score: 201.9, Epsilon: 1.200\n",
      "\n",
      "Episode 18900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.7, Eval Score: 188.3, Epsilon: 1.200\n",
      "\n",
      "Episode 19000/50000, Stage: Full Game\n",
      "Avg. Reward: 57.3, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 19100/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 202.7, Epsilon: 1.200\n",
      "\n",
      "Episode 19200/50000, Stage: Full Game\n",
      "Avg. Reward: 58.5, Eval Score: 196.6, Epsilon: 1.200\n",
      "\n",
      "Episode 19300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 199.6, Epsilon: 1.200\n",
      "\n",
      "Episode 19400/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 198.9, Epsilon: 1.200\n",
      "\n",
      "Episode 19500/50000, Stage: Full Game\n",
      "Avg. Reward: 54.0, Eval Score: 205.3, Epsilon: 1.200\n",
      "\n",
      "Episode 19600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 199.9, Epsilon: 1.200\n",
      "\n",
      "Episode 19700/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 188.1, Epsilon: 1.200\n",
      "\n",
      "Episode 19800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 19900/50000, Stage: Full Game\n",
      "Avg. Reward: 67.5, Eval Score: 198.5, Epsilon: 1.200\n",
      "\n",
      "Episode 20000/50000, Stage: Full Game\n",
      "Avg. Reward: 69.7, Eval Score: 193.8, Epsilon: 1.200\n",
      "\n",
      "Episode 20100/50000, Stage: Full Game\n",
      "Avg. Reward: 62.9, Eval Score: 188.5, Epsilon: 1.200\n",
      "\n",
      "Episode 20200/50000, Stage: Full Game\n",
      "Avg. Reward: 65.1, Eval Score: 203.3, Epsilon: 1.200\n",
      "\n",
      "Episode 20300/50000, Stage: Full Game\n",
      "Avg. Reward: 62.2, Eval Score: 200.6, Epsilon: 1.200\n",
      "\n",
      "Episode 20400/50000, Stage: Full Game\n",
      "Avg. Reward: 55.0, Eval Score: 205.7, Epsilon: 1.200\n",
      "\n",
      "Episode 20500/50000, Stage: Full Game\n",
      "Avg. Reward: 58.8, Eval Score: 191.8, Epsilon: 1.200\n",
      "\n",
      "Episode 20600/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 195.2, Epsilon: 1.200\n",
      "\n",
      "Episode 20700/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 205.5, Epsilon: 1.200\n",
      "\n",
      "Episode 20800/50000, Stage: Full Game\n",
      "Avg. Reward: 58.1, Eval Score: 196.7, Epsilon: 1.200\n",
      "\n",
      "Episode 20900/50000, Stage: Full Game\n",
      "Avg. Reward: 58.5, Eval Score: 200.7, Epsilon: 1.200\n",
      "\n",
      "Episode 21000/50000, Stage: Full Game\n",
      "Avg. Reward: 63.9, Eval Score: 191.7, Epsilon: 1.200\n",
      "\n",
      "Episode 21100/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 192.2, Epsilon: 1.200\n",
      "\n",
      "Episode 21200/50000, Stage: Full Game\n",
      "Avg. Reward: 64.0, Eval Score: 191.7, Epsilon: 1.200\n",
      "\n",
      "Episode 21300/50000, Stage: Full Game\n",
      "Avg. Reward: 57.1, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 21400/50000, Stage: Full Game\n",
      "Avg. Reward: 60.5, Eval Score: 199.0, Epsilon: 1.200\n",
      "\n",
      "Episode 21500/50000, Stage: Full Game\n",
      "Avg. Reward: 60.7, Eval Score: 200.9, Epsilon: 1.200\n",
      "\n",
      "Episode 21600/50000, Stage: Full Game\n",
      "Avg. Reward: 65.4, Eval Score: 198.2, Epsilon: 1.200\n",
      "\n",
      "Episode 21700/50000, Stage: Full Game\n",
      "Avg. Reward: 62.3, Eval Score: 197.0, Epsilon: 1.200\n",
      "\n",
      "Episode 21800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.3, Eval Score: 202.5, Epsilon: 1.200\n",
      "\n",
      "Episode 21900/50000, Stage: Full Game\n",
      "Avg. Reward: 63.1, Eval Score: 201.0, Epsilon: 1.200\n",
      "\n",
      "Episode 22000/50000, Stage: Full Game\n",
      "Avg. Reward: 64.2, Eval Score: 197.1, Epsilon: 1.200\n",
      "\n",
      "Episode 22100/50000, Stage: Full Game\n",
      "Avg. Reward: 59.8, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 22200/50000, Stage: Full Game\n",
      "Avg. Reward: 65.1, Eval Score: 194.6, Epsilon: 1.200\n",
      "\n",
      "Episode 22300/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 189.1, Epsilon: 1.200\n",
      "\n",
      "Episode 22400/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 201.3, Epsilon: 1.200\n",
      "\n",
      "Episode 22500/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 195.9, Epsilon: 1.200\n",
      "\n",
      "Episode 22600/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 206.7, Epsilon: 1.200\n",
      "\n",
      "Episode 22700/50000, Stage: Full Game\n",
      "Avg. Reward: 57.4, Eval Score: 200.3, Epsilon: 1.200\n",
      "\n",
      "Episode 22800/50000, Stage: Full Game\n",
      "Avg. Reward: 57.9, Eval Score: 201.2, Epsilon: 1.200\n",
      "\n",
      "Episode 22900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 191.7, Epsilon: 1.200\n",
      "\n",
      "Episode 23000/50000, Stage: Full Game\n",
      "Avg. Reward: 58.1, Eval Score: 190.5, Epsilon: 1.200\n",
      "\n",
      "Episode 23100/50000, Stage: Full Game\n",
      "Avg. Reward: 58.7, Eval Score: 193.6, Epsilon: 1.200\n",
      "\n",
      "Episode 23200/50000, Stage: Full Game\n",
      "Avg. Reward: 57.6, Eval Score: 205.6, Epsilon: 1.200\n",
      "\n",
      "Episode 23300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.0, Eval Score: 202.5, Epsilon: 1.200\n",
      "\n",
      "Episode 23400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.6, Eval Score: 204.7, Epsilon: 1.200\n",
      "\n",
      "Episode 23500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.8, Eval Score: 210.5, Epsilon: 1.200\n",
      "\n",
      "Episode 23600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 203.1, Epsilon: 1.200\n",
      "\n",
      "Episode 23700/50000, Stage: Full Game\n",
      "Avg. Reward: 61.4, Eval Score: 204.6, Epsilon: 1.200\n",
      "\n",
      "Episode 23800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.8, Eval Score: 198.3, Epsilon: 1.200\n",
      "\n",
      "Episode 23900/50000, Stage: Full Game\n",
      "Avg. Reward: 61.0, Eval Score: 202.0, Epsilon: 1.200\n",
      "\n",
      "Episode 24000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 197.0, Epsilon: 1.200\n",
      "\n",
      "Episode 24100/50000, Stage: Full Game\n",
      "Avg. Reward: 64.0, Eval Score: 202.0, Epsilon: 1.200\n",
      "\n",
      "Episode 24200/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 24300/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 194.3, Epsilon: 1.200\n",
      "\n",
      "Episode 24400/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 193.5, Epsilon: 1.200\n",
      "\n",
      "Episode 24500/50000, Stage: Full Game\n",
      "Avg. Reward: 58.6, Eval Score: 202.7, Epsilon: 1.200\n",
      "\n",
      "Episode 24600/50000, Stage: Full Game\n",
      "Avg. Reward: 57.9, Eval Score: 200.5, Epsilon: 1.200\n",
      "\n",
      "Episode 24700/50000, Stage: Full Game\n",
      "Avg. Reward: 58.8, Eval Score: 202.2, Epsilon: 1.200\n",
      "\n",
      "Episode 24800/50000, Stage: Full Game\n",
      "Avg. Reward: 61.4, Eval Score: 193.2, Epsilon: 1.200\n",
      "\n",
      "Episode 24900/50000, Stage: Full Game\n",
      "Avg. Reward: 63.3, Eval Score: 192.3, Epsilon: 1.200\n",
      "\n",
      "Episode 25000/50000, Stage: Full Game\n",
      "Avg. Reward: 63.5, Eval Score: 197.9, Epsilon: 1.200\n",
      "\n",
      "Episode 25100/50000, Stage: Full Game\n",
      "Avg. Reward: 65.4, Eval Score: 207.0, Epsilon: 1.200\n",
      "\n",
      "Episode 25200/50000, Stage: Full Game\n",
      "Avg. Reward: 60.2, Eval Score: 199.0, Epsilon: 1.200\n",
      "\n",
      "Episode 25300/50000, Stage: Full Game\n",
      "Avg. Reward: 63.2, Eval Score: 207.1, Epsilon: 1.200\n",
      "\n",
      "Episode 25400/50000, Stage: Full Game\n",
      "Avg. Reward: 59.7, Eval Score: 204.9, Epsilon: 1.200\n",
      "\n",
      "Episode 25500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 211.2, Epsilon: 1.200\n",
      "\n",
      "Episode 25600/50000, Stage: Full Game\n",
      "Avg. Reward: 60.6, Eval Score: 203.0, Epsilon: 1.200\n",
      "\n",
      "Episode 25700/50000, Stage: Full Game\n",
      "Avg. Reward: 60.0, Eval Score: 189.9, Epsilon: 1.200\n",
      "\n",
      "Episode 25800/50000, Stage: Full Game\n",
      "Avg. Reward: 61.8, Eval Score: 199.6, Epsilon: 1.200\n",
      "\n",
      "Episode 25900/50000, Stage: Full Game\n",
      "Avg. Reward: 64.4, Eval Score: 198.0, Epsilon: 1.200\n",
      "\n",
      "Episode 26000/50000, Stage: Full Game\n",
      "Avg. Reward: 64.0, Eval Score: 194.8, Epsilon: 1.200\n",
      "\n",
      "Episode 26100/50000, Stage: Full Game\n",
      "Avg. Reward: 59.8, Eval Score: 198.2, Epsilon: 1.200\n",
      "\n",
      "Episode 26200/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 195.8, Epsilon: 1.200\n",
      "\n",
      "Episode 26300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.2, Eval Score: 202.8, Epsilon: 1.200\n",
      "\n",
      "Episode 26400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.5, Eval Score: 196.8, Epsilon: 1.200\n",
      "\n",
      "Episode 26500/50000, Stage: Full Game\n",
      "Avg. Reward: 63.9, Eval Score: 211.9, Epsilon: 1.200\n",
      "\n",
      "Episode 26600/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 191.9, Epsilon: 1.200\n",
      "\n",
      "Episode 26700/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 204.3, Epsilon: 1.200\n",
      "\n",
      "Episode 26800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.1, Eval Score: 203.2, Epsilon: 1.200\n",
      "\n",
      "Episode 26900/50000, Stage: Full Game\n",
      "Avg. Reward: 60.9, Eval Score: 203.8, Epsilon: 1.200\n",
      "\n",
      "Episode 27000/50000, Stage: Full Game\n",
      "Avg. Reward: 61.8, Eval Score: 195.5, Epsilon: 1.200\n",
      "\n",
      "Episode 27100/50000, Stage: Full Game\n",
      "Avg. Reward: 64.8, Eval Score: 201.4, Epsilon: 1.200\n",
      "\n",
      "Episode 27200/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 195.7, Epsilon: 1.200\n",
      "\n",
      "Episode 27300/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 198.9, Epsilon: 1.200\n",
      "\n",
      "Episode 27400/50000, Stage: Full Game\n",
      "Avg. Reward: 60.4, Eval Score: 203.0, Epsilon: 1.200\n",
      "\n",
      "Episode 27500/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 205.1, Epsilon: 1.200\n",
      "\n",
      "Episode 27600/50000, Stage: Full Game\n",
      "Avg. Reward: 58.1, Eval Score: 207.5, Epsilon: 1.200\n",
      "\n",
      "Episode 27700/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 206.4, Epsilon: 1.200\n",
      "\n",
      "Episode 27800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.5, Eval Score: 203.9, Epsilon: 1.200\n",
      "\n",
      "Episode 27900/50000, Stage: Full Game\n",
      "Avg. Reward: 60.6, Eval Score: 208.5, Epsilon: 1.200\n",
      "\n",
      "Episode 28000/50000, Stage: Full Game\n",
      "Avg. Reward: 56.6, Eval Score: 203.9, Epsilon: 1.200\n",
      "\n",
      "Episode 28100/50000, Stage: Full Game\n",
      "Avg. Reward: 56.9, Eval Score: 206.6, Epsilon: 1.200\n",
      "\n",
      "Episode 28200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 204.4, Epsilon: 1.200\n",
      "\n",
      "Episode 28300/50000, Stage: Full Game\n",
      "Avg. Reward: 63.5, Eval Score: 202.7, Epsilon: 1.200\n",
      "\n",
      "Episode 28400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 192.7, Epsilon: 1.200\n",
      "\n",
      "Episode 28500/50000, Stage: Full Game\n",
      "Avg. Reward: 57.1, Eval Score: 199.4, Epsilon: 1.200\n",
      "\n",
      "Episode 28600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.3, Eval Score: 194.6, Epsilon: 1.200\n",
      "\n",
      "Episode 28700/50000, Stage: Full Game\n",
      "Avg. Reward: 63.6, Eval Score: 202.6, Epsilon: 1.200\n",
      "\n",
      "Episode 28800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.2, Eval Score: 196.2, Epsilon: 1.200\n",
      "\n",
      "Episode 28900/50000, Stage: Full Game\n",
      "Avg. Reward: 64.3, Eval Score: 202.8, Epsilon: 1.200\n",
      "\n",
      "Episode 29000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.6, Eval Score: 208.3, Epsilon: 1.200\n",
      "\n",
      "Episode 29100/50000, Stage: Full Game\n",
      "Avg. Reward: 62.5, Eval Score: 191.8, Epsilon: 1.200\n",
      "\n",
      "Episode 29200/50000, Stage: Full Game\n",
      "Avg. Reward: 59.1, Eval Score: 207.3, Epsilon: 1.200\n",
      "\n",
      "Episode 29300/50000, Stage: Full Game\n",
      "Avg. Reward: 60.9, Eval Score: 203.8, Epsilon: 1.200\n",
      "\n",
      "Episode 29400/50000, Stage: Full Game\n",
      "Avg. Reward: 56.7, Eval Score: 200.6, Epsilon: 1.200\n",
      "\n",
      "Episode 29500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 200.3, Epsilon: 1.200\n",
      "\n",
      "Episode 29600/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 201.2, Epsilon: 1.200\n",
      "\n",
      "Episode 29700/50000, Stage: Full Game\n",
      "Avg. Reward: 64.3, Eval Score: 200.0, Epsilon: 1.200\n",
      "\n",
      "Episode 29800/50000, Stage: Full Game\n",
      "Avg. Reward: 63.8, Eval Score: 201.5, Epsilon: 1.200\n",
      "\n",
      "Episode 29900/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 200.8, Epsilon: 1.200\n",
      "\n",
      "Episode 30000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.7, Eval Score: 202.4, Epsilon: 1.200\n",
      "\n",
      "Episode 30100/50000, Stage: Full Game\n",
      "Avg. Reward: 65.5, Eval Score: 196.1, Epsilon: 1.200\n",
      "\n",
      "Episode 30200/50000, Stage: Full Game\n",
      "Avg. Reward: 57.4, Eval Score: 198.6, Epsilon: 1.200\n",
      "\n",
      "Episode 30300/50000, Stage: Full Game\n",
      "Avg. Reward: 59.0, Eval Score: 201.7, Epsilon: 1.200\n",
      "\n",
      "Episode 30400/50000, Stage: Full Game\n",
      "Avg. Reward: 65.8, Eval Score: 204.8, Epsilon: 1.200\n",
      "\n",
      "Episode 30500/50000, Stage: Full Game\n",
      "Avg. Reward: 59.0, Eval Score: 203.7, Epsilon: 1.200\n",
      "\n",
      "Episode 30600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.1, Eval Score: 200.7, Epsilon: 1.200\n",
      "\n",
      "Episode 30700/50000, Stage: Full Game\n",
      "Avg. Reward: 59.5, Eval Score: 194.8, Epsilon: 1.200\n",
      "\n",
      "Episode 30800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 201.4, Epsilon: 1.200\n",
      "\n",
      "Episode 30900/50000, Stage: Full Game\n",
      "Avg. Reward: 57.2, Eval Score: 197.6, Epsilon: 1.200\n",
      "\n",
      "Episode 31000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.0, Eval Score: 194.4, Epsilon: 1.200\n",
      "\n",
      "Episode 31100/50000, Stage: Full Game\n",
      "Avg. Reward: 63.3, Eval Score: 211.5, Epsilon: 1.200\n",
      "\n",
      "Episode 31200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.3, Eval Score: 199.7, Epsilon: 1.200\n",
      "\n",
      "Episode 31300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.7, Eval Score: 199.4, Epsilon: 1.200\n",
      "\n",
      "Episode 31400/50000, Stage: Full Game\n",
      "Avg. Reward: 62.9, Eval Score: 206.7, Epsilon: 1.200\n",
      "\n",
      "Episode 31500/50000, Stage: Full Game\n",
      "Avg. Reward: 57.6, Eval Score: 207.5, Epsilon: 1.200\n",
      "\n",
      "Episode 31600/50000, Stage: Full Game\n",
      "Avg. Reward: 60.9, Eval Score: 206.2, Epsilon: 1.200\n",
      "\n",
      "Episode 31700/50000, Stage: Full Game\n",
      "Avg. Reward: 63.5, Eval Score: 208.7, Epsilon: 1.200\n",
      "\n",
      "Episode 31800/50000, Stage: Full Game\n",
      "Avg. Reward: 64.6, Eval Score: 192.9, Epsilon: 1.200\n",
      "\n",
      "Episode 31900/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 202.2, Epsilon: 1.200\n",
      "\n",
      "Episode 32000/50000, Stage: Full Game\n",
      "Avg. Reward: 62.8, Eval Score: 200.8, Epsilon: 1.200\n",
      "\n",
      "Episode 32100/50000, Stage: Full Game\n",
      "Avg. Reward: 66.3, Eval Score: 196.2, Epsilon: 1.200\n",
      "\n",
      "Episode 32200/50000, Stage: Full Game\n",
      "Avg. Reward: 56.9, Eval Score: 205.0, Epsilon: 1.200\n",
      "\n",
      "Episode 32300/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 195.1, Epsilon: 1.200\n",
      "\n",
      "Episode 32400/50000, Stage: Full Game\n",
      "Avg. Reward: 64.4, Eval Score: 199.2, Epsilon: 1.200\n",
      "\n",
      "Episode 32500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 196.9, Epsilon: 1.200\n",
      "\n",
      "Episode 32600/50000, Stage: Full Game\n",
      "Avg. Reward: 60.0, Eval Score: 209.2, Epsilon: 1.200\n",
      "\n",
      "Episode 32700/50000, Stage: Full Game\n",
      "Avg. Reward: 63.0, Eval Score: 194.3, Epsilon: 1.200\n",
      "\n",
      "Episode 32800/50000, Stage: Full Game\n",
      "Avg. Reward: 63.0, Eval Score: 206.2, Epsilon: 1.200\n",
      "\n",
      "Episode 32900/50000, Stage: Full Game\n",
      "Avg. Reward: 57.5, Eval Score: 208.7, Epsilon: 1.200\n",
      "\n",
      "Episode 33000/50000, Stage: Full Game\n",
      "Avg. Reward: 65.1, Eval Score: 203.6, Epsilon: 1.200\n",
      "\n",
      "Episode 33100/50000, Stage: Full Game\n",
      "Avg. Reward: 60.0, Eval Score: 198.7, Epsilon: 1.200\n",
      "\n",
      "Episode 33200/50000, Stage: Full Game\n",
      "Avg. Reward: 59.8, Eval Score: 201.2, Epsilon: 1.200\n",
      "\n",
      "Episode 33300/50000, Stage: Full Game\n",
      "Avg. Reward: 63.6, Eval Score: 199.5, Epsilon: 1.200\n",
      "\n",
      "Episode 33400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.4, Eval Score: 208.0, Epsilon: 1.200\n",
      "\n",
      "Episode 33500/50000, Stage: Full Game\n",
      "Avg. Reward: 56.8, Eval Score: 203.0, Epsilon: 1.200\n",
      "\n",
      "Episode 33600/50000, Stage: Full Game\n",
      "Avg. Reward: 60.8, Eval Score: 203.8, Epsilon: 1.200\n",
      "\n",
      "Episode 33700/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 203.4, Epsilon: 1.200\n",
      "\n",
      "Episode 33800/50000, Stage: Full Game\n",
      "Avg. Reward: 57.7, Eval Score: 207.8, Epsilon: 1.200\n",
      "\n",
      "Episode 33900/50000, Stage: Full Game\n",
      "Avg. Reward: 60.7, Eval Score: 203.6, Epsilon: 1.200\n",
      "\n",
      "Episode 34000/50000, Stage: Full Game\n",
      "Avg. Reward: 65.7, Eval Score: 193.7, Epsilon: 1.200\n",
      "\n",
      "Episode 34100/50000, Stage: Full Game\n",
      "Avg. Reward: 58.3, Eval Score: 202.3, Epsilon: 1.200\n",
      "\n",
      "Episode 34200/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 192.5, Epsilon: 1.200\n",
      "\n",
      "Episode 34300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.4, Eval Score: 204.4, Epsilon: 1.200\n",
      "\n",
      "Episode 34400/50000, Stage: Full Game\n",
      "Avg. Reward: 63.2, Eval Score: 206.2, Epsilon: 1.200\n",
      "\n",
      "Episode 34500/50000, Stage: Full Game\n",
      "Avg. Reward: 62.1, Eval Score: 208.1, Epsilon: 1.200\n",
      "\n",
      "Episode 34600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 197.9, Epsilon: 1.200\n",
      "\n",
      "Episode 34700/50000, Stage: Full Game\n",
      "Avg. Reward: 56.6, Eval Score: 208.0, Epsilon: 1.200\n",
      "\n",
      "Episode 34800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.1, Eval Score: 205.4, Epsilon: 1.200\n",
      "\n",
      "Episode 34900/50000, Stage: Full Game\n",
      "Avg. Reward: 63.2, Eval Score: 201.0, Epsilon: 1.200\n",
      "\n",
      "Episode 35000/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 198.8, Epsilon: 1.200\n",
      "\n",
      "Episode 35100/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 207.7, Epsilon: 1.200\n",
      "\n",
      "Episode 35200/50000, Stage: Full Game\n",
      "Avg. Reward: 58.0, Eval Score: 206.3, Epsilon: 1.200\n",
      "\n",
      "Episode 35300/50000, Stage: Full Game\n",
      "Avg. Reward: 60.2, Eval Score: 198.6, Epsilon: 1.200\n",
      "\n",
      "Episode 35400/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 202.9, Epsilon: 1.200\n",
      "\n",
      "Episode 35500/50000, Stage: Full Game\n",
      "Avg. Reward: 62.8, Eval Score: 205.9, Epsilon: 1.200\n",
      "\n",
      "Episode 35600/50000, Stage: Full Game\n",
      "Avg. Reward: 65.1, Eval Score: 194.8, Epsilon: 1.200\n",
      "\n",
      "Episode 35700/50000, Stage: Full Game\n",
      "Avg. Reward: 60.9, Eval Score: 199.9, Epsilon: 1.200\n",
      "\n",
      "Episode 35800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.0, Eval Score: 204.3, Epsilon: 1.200\n",
      "\n",
      "Episode 35900/50000, Stage: Full Game\n",
      "Avg. Reward: 59.9, Eval Score: 207.1, Epsilon: 1.200\n",
      "\n",
      "Episode 36000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.1, Eval Score: 200.2, Epsilon: 1.200\n",
      "\n",
      "Episode 36100/50000, Stage: Full Game\n",
      "Avg. Reward: 63.7, Eval Score: 197.2, Epsilon: 1.200\n",
      "\n",
      "Episode 36200/50000, Stage: Full Game\n",
      "Avg. Reward: 59.8, Eval Score: 210.0, Epsilon: 1.200\n",
      "\n",
      "Episode 36300/50000, Stage: Full Game\n",
      "Avg. Reward: 64.4, Eval Score: 211.2, Epsilon: 1.200\n",
      "\n",
      "Episode 36400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.7, Eval Score: 201.6, Epsilon: 1.200\n",
      "\n",
      "Episode 36500/50000, Stage: Full Game\n",
      "Avg. Reward: 58.0, Eval Score: 209.4, Epsilon: 1.200\n",
      "\n",
      "Episode 36600/50000, Stage: Full Game\n",
      "Avg. Reward: 57.3, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 36700/50000, Stage: Full Game\n",
      "Avg. Reward: 59.2, Eval Score: 212.8, Epsilon: 1.200\n",
      "\n",
      "Episode 36800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 200.0, Epsilon: 1.200\n",
      "\n",
      "Episode 36900/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 210.7, Epsilon: 1.200\n",
      "\n",
      "Episode 37000/50000, Stage: Full Game\n",
      "Avg. Reward: 56.5, Eval Score: 209.8, Epsilon: 1.200\n",
      "\n",
      "Episode 37100/50000, Stage: Full Game\n",
      "Avg. Reward: 65.5, Eval Score: 205.3, Epsilon: 1.200\n",
      "\n",
      "Episode 37200/50000, Stage: Full Game\n",
      "Avg. Reward: 55.6, Eval Score: 196.5, Epsilon: 1.200\n",
      "\n",
      "Episode 37300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.7, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 37400/50000, Stage: Full Game\n",
      "Avg. Reward: 63.2, Eval Score: 209.5, Epsilon: 1.200\n",
      "\n",
      "Episode 37500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 211.3, Epsilon: 1.200\n",
      "\n",
      "Episode 37600/50000, Stage: Full Game\n",
      "Avg. Reward: 61.7, Eval Score: 211.9, Epsilon: 1.200\n",
      "\n",
      "Episode 37700/50000, Stage: Full Game\n",
      "Avg. Reward: 64.2, Eval Score: 207.9, Epsilon: 1.200\n",
      "\n",
      "Episode 37800/50000, Stage: Full Game\n",
      "Avg. Reward: 66.1, Eval Score: 209.2, Epsilon: 1.200\n",
      "\n",
      "Episode 37900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.2, Eval Score: 203.6, Epsilon: 1.200\n",
      "\n",
      "Episode 38000/50000, Stage: Full Game\n",
      "Avg. Reward: 64.4, Eval Score: 204.2, Epsilon: 1.200\n",
      "\n",
      "Episode 38100/50000, Stage: Full Game\n",
      "Avg. Reward: 57.6, Eval Score: 216.8, Epsilon: 1.200\n",
      "\n",
      "Episode 38200/50000, Stage: Full Game\n",
      "Avg. Reward: 62.3, Eval Score: 209.9, Epsilon: 1.200\n",
      "\n",
      "Episode 38300/50000, Stage: Full Game\n",
      "Avg. Reward: 56.9, Eval Score: 204.5, Epsilon: 1.200\n",
      "\n",
      "Episode 38400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 199.5, Epsilon: 1.200\n",
      "\n",
      "Episode 38500/50000, Stage: Full Game\n",
      "Avg. Reward: 60.2, Eval Score: 199.5, Epsilon: 1.200\n",
      "\n",
      "Episode 38600/50000, Stage: Full Game\n",
      "Avg. Reward: 56.6, Eval Score: 205.4, Epsilon: 1.200\n",
      "\n",
      "Episode 38700/50000, Stage: Full Game\n",
      "Avg. Reward: 56.3, Eval Score: 212.0, Epsilon: 1.200\n",
      "\n",
      "Episode 38800/50000, Stage: Full Game\n",
      "Avg. Reward: 60.6, Eval Score: 205.0, Epsilon: 1.200\n",
      "\n",
      "Episode 38900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.3, Eval Score: 201.7, Epsilon: 1.200\n",
      "\n",
      "Episode 39000/50000, Stage: Full Game\n",
      "Avg. Reward: 64.2, Eval Score: 214.3, Epsilon: 1.200\n",
      "\n",
      "Episode 39100/50000, Stage: Full Game\n",
      "Avg. Reward: 63.7, Eval Score: 197.4, Epsilon: 1.200\n",
      "\n",
      "Episode 39200/50000, Stage: Full Game\n",
      "Avg. Reward: 60.5, Eval Score: 203.6, Epsilon: 1.200\n",
      "\n",
      "Episode 39300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.3, Eval Score: 209.2, Epsilon: 1.200\n",
      "\n",
      "Episode 39400/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 196.0, Epsilon: 1.200\n",
      "\n",
      "Episode 39500/50000, Stage: Full Game\n",
      "Avg. Reward: 60.2, Eval Score: 213.5, Epsilon: 1.200\n",
      "\n",
      "Episode 39600/50000, Stage: Full Game\n",
      "Avg. Reward: 64.7, Eval Score: 203.0, Epsilon: 1.200\n",
      "\n",
      "Episode 39700/50000, Stage: Full Game\n",
      "Avg. Reward: 59.5, Eval Score: 203.4, Epsilon: 1.200\n",
      "\n",
      "Episode 39800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 201.2, Epsilon: 1.200\n",
      "\n",
      "Episode 39900/50000, Stage: Full Game\n",
      "Avg. Reward: 63.3, Eval Score: 203.3, Epsilon: 1.200\n",
      "\n",
      "Episode 40000/50000, Stage: Full Game\n",
      "Avg. Reward: 65.5, Eval Score: 210.4, Epsilon: 1.200\n",
      "\n",
      "Episode 40100/50000, Stage: Full Game\n",
      "Avg. Reward: 61.0, Eval Score: 201.3, Epsilon: 1.200\n",
      "\n",
      "Episode 40200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.3, Eval Score: 212.4, Epsilon: 1.200\n",
      "\n",
      "Episode 40300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 205.4, Epsilon: 1.200\n",
      "\n",
      "Episode 40400/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 201.1, Epsilon: 1.200\n",
      "\n",
      "Episode 40500/50000, Stage: Full Game\n",
      "Avg. Reward: 57.2, Eval Score: 209.1, Epsilon: 1.200\n",
      "\n",
      "Episode 40600/50000, Stage: Full Game\n",
      "Avg. Reward: 64.3, Eval Score: 204.3, Epsilon: 1.200\n",
      "\n",
      "Episode 40700/50000, Stage: Full Game\n",
      "Avg. Reward: 60.0, Eval Score: 196.5, Epsilon: 1.200\n",
      "\n",
      "Episode 40800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.7, Eval Score: 207.3, Epsilon: 1.200\n",
      "\n",
      "Episode 40900/50000, Stage: Full Game\n",
      "Avg. Reward: 68.0, Eval Score: 212.2, Epsilon: 1.200\n",
      "\n",
      "Episode 41000/50000, Stage: Full Game\n",
      "Avg. Reward: 64.7, Eval Score: 208.8, Epsilon: 1.200\n",
      "\n",
      "Episode 41100/50000, Stage: Full Game\n",
      "Avg. Reward: 59.6, Eval Score: 205.2, Epsilon: 1.200\n",
      "\n",
      "Episode 41200/50000, Stage: Full Game\n",
      "Avg. Reward: 60.9, Eval Score: 215.4, Epsilon: 1.200\n",
      "\n",
      "Episode 41300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 204.0, Epsilon: 1.200\n",
      "\n",
      "Episode 41400/50000, Stage: Full Game\n",
      "Avg. Reward: 57.8, Eval Score: 205.0, Epsilon: 1.200\n",
      "\n",
      "Episode 41500/50000, Stage: Full Game\n",
      "Avg. Reward: 63.4, Eval Score: 211.2, Epsilon: 1.200\n",
      "\n",
      "Episode 41600/50000, Stage: Full Game\n",
      "Avg. Reward: 57.7, Eval Score: 204.4, Epsilon: 1.200\n",
      "\n",
      "Episode 41700/50000, Stage: Full Game\n",
      "Avg. Reward: 60.9, Eval Score: 211.9, Epsilon: 1.200\n",
      "\n",
      "Episode 41800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.6, Eval Score: 205.1, Epsilon: 1.200\n",
      "\n",
      "Episode 41900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.3, Eval Score: 208.6, Epsilon: 1.200\n",
      "\n",
      "Episode 42000/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 209.6, Epsilon: 1.200\n",
      "\n",
      "Episode 42100/50000, Stage: Full Game\n",
      "Avg. Reward: 62.9, Eval Score: 206.9, Epsilon: 1.200\n",
      "\n",
      "Episode 42200/50000, Stage: Full Game\n",
      "Avg. Reward: 60.4, Eval Score: 210.3, Epsilon: 1.200\n",
      "\n",
      "Episode 42300/50000, Stage: Full Game\n",
      "Avg. Reward: 59.1, Eval Score: 214.0, Epsilon: 1.200\n",
      "\n",
      "Episode 42400/50000, Stage: Full Game\n",
      "Avg. Reward: 60.8, Eval Score: 201.5, Epsilon: 1.200\n",
      "\n",
      "Episode 42500/50000, Stage: Full Game\n",
      "Avg. Reward: 61.7, Eval Score: 211.7, Epsilon: 1.200\n",
      "\n",
      "Episode 42600/50000, Stage: Full Game\n",
      "Avg. Reward: 63.5, Eval Score: 210.0, Epsilon: 1.200\n",
      "\n",
      "Episode 42700/50000, Stage: Full Game\n",
      "Avg. Reward: 63.4, Eval Score: 204.0, Epsilon: 1.200\n",
      "\n",
      "Episode 42800/50000, Stage: Full Game\n",
      "Avg. Reward: 56.9, Eval Score: 214.1, Epsilon: 1.200\n",
      "\n",
      "Episode 42900/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 206.2, Epsilon: 1.200\n",
      "\n",
      "Episode 43000/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 199.5, Epsilon: 1.200\n",
      "\n",
      "Episode 43100/50000, Stage: Full Game\n",
      "Avg. Reward: 60.7, Eval Score: 206.2, Epsilon: 1.200\n",
      "\n",
      "Episode 43200/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 209.7, Epsilon: 1.200\n",
      "\n",
      "Episode 43300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.1, Eval Score: 197.9, Epsilon: 1.200\n",
      "\n",
      "Episode 43400/50000, Stage: Full Game\n",
      "Avg. Reward: 58.7, Eval Score: 204.8, Epsilon: 1.200\n",
      "\n",
      "Episode 43500/50000, Stage: Full Game\n",
      "Avg. Reward: 58.6, Eval Score: 212.6, Epsilon: 1.200\n",
      "\n",
      "Episode 43600/50000, Stage: Full Game\n",
      "Avg. Reward: 63.8, Eval Score: 195.0, Epsilon: 1.200\n",
      "\n",
      "Episode 43700/50000, Stage: Full Game\n",
      "Avg. Reward: 64.8, Eval Score: 215.4, Epsilon: 1.200\n",
      "\n",
      "Episode 43800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.4, Eval Score: 210.7, Epsilon: 1.200\n",
      "\n",
      "Episode 43900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 208.9, Epsilon: 1.200\n",
      "\n",
      "Episode 44000/50000, Stage: Full Game\n",
      "Avg. Reward: 62.5, Eval Score: 207.2, Epsilon: 1.200\n",
      "\n",
      "Episode 44100/50000, Stage: Full Game\n",
      "Avg. Reward: 63.2, Eval Score: 205.9, Epsilon: 1.200\n",
      "\n",
      "Episode 44200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.4, Eval Score: 211.7, Epsilon: 1.200\n",
      "\n",
      "Episode 44300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 206.4, Epsilon: 1.200\n",
      "\n",
      "Episode 44400/50000, Stage: Full Game\n",
      "Avg. Reward: 60.1, Eval Score: 205.9, Epsilon: 1.200\n",
      "\n",
      "Episode 44500/50000, Stage: Full Game\n",
      "Avg. Reward: 58.3, Eval Score: 208.5, Epsilon: 1.200\n",
      "\n",
      "Episode 44600/50000, Stage: Full Game\n",
      "Avg. Reward: 65.0, Eval Score: 211.0, Epsilon: 1.200\n",
      "\n",
      "Episode 44700/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 196.3, Epsilon: 1.200\n",
      "\n",
      "Episode 44800/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 209.6, Epsilon: 1.200\n",
      "\n",
      "Episode 44900/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 203.3, Epsilon: 1.200\n",
      "\n",
      "Episode 45000/50000, Stage: Full Game\n",
      "Avg. Reward: 66.2, Eval Score: 207.1, Epsilon: 1.200\n",
      "\n",
      "Episode 45100/50000, Stage: Full Game\n",
      "Avg. Reward: 57.8, Eval Score: 203.7, Epsilon: 1.200\n",
      "\n",
      "Episode 45200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 212.7, Epsilon: 1.200\n",
      "\n",
      "Episode 45300/50000, Stage: Full Game\n",
      "Avg. Reward: 60.4, Eval Score: 209.9, Epsilon: 1.200\n",
      "\n",
      "Episode 45400/50000, Stage: Full Game\n",
      "Avg. Reward: 55.9, Eval Score: 199.5, Epsilon: 1.200\n",
      "\n",
      "Episode 45500/50000, Stage: Full Game\n",
      "Avg. Reward: 58.4, Eval Score: 206.0, Epsilon: 1.200\n",
      "\n",
      "Episode 45600/50000, Stage: Full Game\n",
      "Avg. Reward: 63.6, Eval Score: 204.9, Epsilon: 1.200\n",
      "\n",
      "Episode 45700/50000, Stage: Full Game\n",
      "Avg. Reward: 62.1, Eval Score: 197.1, Epsilon: 1.200\n",
      "\n",
      "Episode 45800/50000, Stage: Full Game\n",
      "Avg. Reward: 64.3, Eval Score: 209.4, Epsilon: 1.200\n",
      "\n",
      "Episode 45900/50000, Stage: Full Game\n",
      "Avg. Reward: 68.6, Eval Score: 201.8, Epsilon: 1.200\n",
      "\n",
      "Episode 46000/50000, Stage: Full Game\n",
      "Avg. Reward: 56.4, Eval Score: 215.0, Epsilon: 1.200\n",
      "\n",
      "Episode 46100/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 205.2, Epsilon: 1.200\n",
      "\n",
      "Episode 46200/50000, Stage: Full Game\n",
      "Avg. Reward: 61.8, Eval Score: 210.9, Epsilon: 1.200\n",
      "\n",
      "Episode 46300/50000, Stage: Full Game\n",
      "Avg. Reward: 59.7, Eval Score: 203.2, Epsilon: 1.200\n",
      "\n",
      "Episode 46400/50000, Stage: Full Game\n",
      "Avg. Reward: 57.0, Eval Score: 213.9, Epsilon: 1.200\n",
      "\n",
      "Episode 46500/50000, Stage: Full Game\n",
      "Avg. Reward: 58.9, Eval Score: 207.4, Epsilon: 1.200\n",
      "\n",
      "Episode 46600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 204.5, Epsilon: 1.200\n",
      "\n",
      "Episode 46700/50000, Stage: Full Game\n",
      "Avg. Reward: 59.5, Eval Score: 204.6, Epsilon: 1.200\n",
      "\n",
      "Episode 46800/50000, Stage: Full Game\n",
      "Avg. Reward: 63.0, Eval Score: 209.5, Epsilon: 1.200\n",
      "\n",
      "Episode 46900/50000, Stage: Full Game\n",
      "Avg. Reward: 55.9, Eval Score: 215.4, Epsilon: 1.200\n",
      "\n",
      "Episode 47000/50000, Stage: Full Game\n",
      "Avg. Reward: 60.1, Eval Score: 213.1, Epsilon: 1.200\n",
      "\n",
      "Episode 47100/50000, Stage: Full Game\n",
      "Avg. Reward: 60.6, Eval Score: 202.1, Epsilon: 1.200\n",
      "\n",
      "Episode 47200/50000, Stage: Full Game\n",
      "Avg. Reward: 62.4, Eval Score: 213.0, Epsilon: 1.200\n",
      "\n",
      "Episode 47300/50000, Stage: Full Game\n",
      "Avg. Reward: 61.4, Eval Score: 206.9, Epsilon: 1.200\n",
      "\n",
      "Episode 47400/50000, Stage: Full Game\n",
      "Avg. Reward: 64.9, Eval Score: 201.3, Epsilon: 1.200\n",
      "\n",
      "Episode 47500/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 210.3, Epsilon: 1.200\n",
      "\n",
      "Episode 47600/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 219.5, Epsilon: 1.200\n",
      "\n",
      "Episode 47700/50000, Stage: Full Game\n",
      "Avg. Reward: 61.2, Eval Score: 212.5, Epsilon: 1.200\n",
      "\n",
      "Episode 47800/50000, Stage: Full Game\n",
      "Avg. Reward: 63.3, Eval Score: 203.9, Epsilon: 1.200\n",
      "\n",
      "Episode 47900/50000, Stage: Full Game\n",
      "Avg. Reward: 61.0, Eval Score: 203.7, Epsilon: 1.200\n",
      "\n",
      "Episode 48000/50000, Stage: Full Game\n",
      "Avg. Reward: 57.9, Eval Score: 210.7, Epsilon: 1.200\n",
      "\n",
      "Episode 48100/50000, Stage: Full Game\n",
      "Avg. Reward: 61.6, Eval Score: 211.2, Epsilon: 1.200\n",
      "\n",
      "Episode 48200/50000, Stage: Full Game\n",
      "Avg. Reward: 63.3, Eval Score: 199.8, Epsilon: 1.200\n",
      "\n",
      "Episode 48300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.5, Eval Score: 209.5, Epsilon: 1.200\n",
      "\n",
      "Episode 48400/50000, Stage: Full Game\n",
      "Avg. Reward: 60.6, Eval Score: 202.2, Epsilon: 1.200\n",
      "\n",
      "Episode 48500/50000, Stage: Full Game\n",
      "Avg. Reward: 60.3, Eval Score: 212.4, Epsilon: 1.200\n",
      "\n",
      "Episode 48600/50000, Stage: Full Game\n",
      "Avg. Reward: 61.9, Eval Score: 208.8, Epsilon: 1.200\n",
      "\n",
      "Episode 48700/50000, Stage: Full Game\n",
      "Avg. Reward: 57.4, Eval Score: 197.1, Epsilon: 1.200\n",
      "\n",
      "Episode 48800/50000, Stage: Full Game\n",
      "Avg. Reward: 59.6, Eval Score: 205.3, Epsilon: 1.200\n",
      "\n",
      "Episode 48900/50000, Stage: Full Game\n",
      "Avg. Reward: 60.7, Eval Score: 208.6, Epsilon: 1.200\n",
      "\n",
      "Episode 49000/50000, Stage: Full Game\n",
      "Avg. Reward: 62.6, Eval Score: 205.9, Epsilon: 1.200\n",
      "\n",
      "Episode 49100/50000, Stage: Full Game\n",
      "Avg. Reward: 58.6, Eval Score: 194.5, Epsilon: 1.200\n",
      "\n",
      "Episode 49200/50000, Stage: Full Game\n",
      "Avg. Reward: 59.2, Eval Score: 208.9, Epsilon: 1.200\n",
      "\n",
      "Episode 49300/50000, Stage: Full Game\n",
      "Avg. Reward: 58.2, Eval Score: 204.7, Epsilon: 1.200\n",
      "\n",
      "Episode 49400/50000, Stage: Full Game\n",
      "Avg. Reward: 65.6, Eval Score: 203.5, Epsilon: 1.200\n",
      "\n",
      "Episode 49500/50000, Stage: Full Game\n",
      "Avg. Reward: 59.3, Eval Score: 213.4, Epsilon: 1.200\n",
      "\n",
      "Episode 49600/50000, Stage: Full Game\n",
      "Avg. Reward: 62.0, Eval Score: 198.8, Epsilon: 1.200\n",
      "\n",
      "Episode 49700/50000, Stage: Full Game\n",
      "Avg. Reward: 58.4, Eval Score: 205.2, Epsilon: 1.200\n",
      "\n",
      "Episode 49800/50000, Stage: Full Game\n",
      "Avg. Reward: 55.9, Eval Score: 210.7, Epsilon: 1.200\n",
      "\n",
      "Episode 49900/50000, Stage: Full Game\n",
      "Avg. Reward: 59.3, Eval Score: 206.1, Epsilon: 1.200\n",
      "\n",
      "Episode 50000/50000, Stage: Full Game\n",
      "Avg. Reward: 61.5, Eval Score: 214.8, Epsilon: 1.200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = YahtzeeEnv()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 50\n",
    "random.seed(seed)  # Python random module\n",
    "np.random.seed(seed)  # NumPy\n",
    "torch.manual_seed(seed)  # PyTorch CPU\n",
    "torch.cuda.manual_seed(seed)  # PyTorch GPU (if available)\n",
    "torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "torch.backends.cudnn.benchmark = False  # Disables auto-tuning for convolutions\n",
    "\n",
    "trainer = YahtzeeTrainerWithCurriculum(env, device)\n",
    "history = trainer.train(eval_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b4b8d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T09:55:39.310144Z",
     "iopub.status.busy": "2025-04-07T09:55:39.309787Z",
     "iopub.status.idle": "2025-04-07T09:55:39.315606Z",
     "shell.execute_reply": "2025-04-07T09:55:39.314746Z"
    },
    "papermill": {
     "duration": 0.031425,
     "end_time": "2025-04-07T09:55:39.316694",
     "exception": false,
     "start_time": "2025-04-07T09:55:39.285269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "torch.save(trainer.agent.target_intuition_net.state_dict(), \"/kaggle/working/target_intuition_net.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 279499,
     "modelInstanceId": 258262,
     "sourceId": 302473,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 289494,
     "modelInstanceId": 268473,
     "sourceId": 318166,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5524.847145,
   "end_time": "2025-04-07T09:55:41.469244",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T08:23:36.622099",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
